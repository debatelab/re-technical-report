[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Assessing a Formal Model of Reflective Equilibrium",
    "section": "",
    "text": "Preface\nThis report summarizes findings of analysing a formal model of the method of reflective equilibrium. The model is based on Beisbart, Betz, and Brun (2021)."
  },
  {
    "objectID": "index.html#reproducibility",
    "href": "index.html#reproducibility",
    "title": "Assessing a Formal Model of Reflective Equilibrium",
    "section": "Reproducibility",
    "text": "Reproducibility\nAll findings and the underlying data can be reproduced by using the Python implementation of the model. The data that the model produced can be found here. For each chapter you will find here a Jupyter notebook whose execution produces all analysis results."
  },
  {
    "objectID": "index.html#licence",
    "href": "index.html#licence",
    "title": "Assessing a Formal Model of Reflective Equilibrium",
    "section": "Licence",
    "text": "Licence\nThis report is licenced under …"
  },
  {
    "objectID": "index.html#credits",
    "href": "index.html#credits",
    "title": "Assessing a Formal Model of Reflective Equilibrium",
    "section": "Credits",
    "text": "Credits\nEarlier versions of this report were discussed on several occasions with all members of the research project ‘How far does Reflective Equilibrium Take us? Investigating the Power of a Philosophical Method’ (SNSF grant 182854 and German Research Foundation grant 412679086). We thank, in particular, Claus Beisbart, Gregor Betz, Georg Brun, Alexander Koch and Richard Lohse for their helpful comments, which helped to improve this report considerably.\n\n\n\n\nBeisbart, Claus, Gregor Betz, and Georg Brun. 2021. “Making Reflective Equlibrium Precise: A Formal Model.” Ergo 8 (0). https://doi.org/10.3998/ergo.1152."
  },
  {
    "objectID": "intro.html#sec-mod",
    "href": "intro.html#sec-mod",
    "title": "1  Introduction",
    "section": "1.1 Modelling reflective equilibration",
    "text": "1.1 Modelling reflective equilibration\nReflective equilibrium is commonly understood as a method of justification, in which an epistemic subject iteratively adjusts its epistemic state in a process of equilibration until a state of reflective equilibrium is reached. In this final state, the agent’s belief system is supposed to be justified to the extent that it satifies various pragmatic-epistemc objectives, e.g., (internal) coherence.\nBeisbart, Betz, and Brun (2021) model this process of reflective equilibration and the underlying axiology of equilibrium states in the following way.2\nThe agent’s epistemic state is modelled as a tuple \\((\\mathcal{C}, \\mathcal{T})\\), which comprises their accepted commitments \\(\\mathcal{C}\\) and a theory \\(\\mathcal{T}\\). Both are represented by sets of sentences from a finite pool of sentences \\(S\\), which is closed under negation.\nThe equilibration process is modelled as a mutual adjustment of the theory and the agent’s commitments to improve the epistemic state as measured by an achievement function \\(Z\\) (see Figure Figure 1.1). The agent starts with a set of initial commitments \\(\\mathcal{C}_0\\). Then, a theory \\(\\mathcal{T}_0\\) is chosen that systematizes \\(\\mathcal{C}_0\\). This initial state \\((\\mathcal{C}_0, \\mathcal{T}_0)\\) is then adjusted by searching for a new set of commitments that performs better in terms of the overall achievement \\(Z\\). This process of adjusting the current epistemic state by choosing a new theory (or new commitments, respectively) goes on until no further improvement is gained any more.\n\n\n\nFigure 1.1: Illustrative diagram of the formal model. The epistemic state, which consists of a set of commitments and a theory, is subject to operationalized desiderata for RE states (bold arrows). Rules for alternating adjustments of commitments and theory specify a process of equilibration that sets out from initial commitments.\n\n\nThe achievement function \\(Z\\) models the underlying axiology and is based on the three different desiderata faithfulness, systematicity and account. Their role is illustrated by bold arrows in Figure Figure 1.1.3\nThe desideratum of faithfulness demands that current commitments should not deviate too much from the initial commitments \\(\\mathcal{C}_{0}\\). There are two motivations for this constraint (Beisbart, Betz, and Brun 2021, 447). A resemblance of the current commitments to \\(\\mathcal{C}_{0}\\) contributes to the justification of the resulting state to the extent that initial commitments have some independent credibility. Additionally, the sentences in \\(\\mathcal{C}_{0}\\) represent a specification of the topic under consideration. Deviating too much from \\(\\mathcal{C}_{0}\\) courts the danger of changing the topic. Faithfulness \\(F(\\mathcal{C}\\,\\vert\\, \\mathcal{C}_{0})\\) is operationalized in the model by measuring the distance of the current commitments to the initial commitments.4\nThe role of the theory \\(\\mathcal{T}\\) is to systematize the commitments \\(\\mathcal{C}\\). One suggestion to explicate this idea is to ask whether the theory implies the commitments. The account \\(A(\\mathcal{C}, \\mathcal{T})\\) measures how well the commitments \\(\\mathcal{C}\\) fit to what is implied by the theory \\(\\mathcal{T}\\). More specifically, \\(A(\\mathcal{C}, T)\\) is based on measuring the distance between \\(\\mathcal{C}\\) and the set of \\(\\mathcal{T}\\)’s implications.\nTo that end, we need to know how the sentences in \\(S\\) are inferentially connected. The inferential relationships are modelled by dialectical structures based on the theory of dialectical structures (Betz 2010, 2013). A dialectical structure \\(\\tau\\) is a set of deductively valid arguments \\(\\mathcal{A}\\) and their “inferential” relationships to each other. For instance, an argument with two premises \\(s_i, s_j\\) (\\(\\in \\mathcal{S}\\)) and a conclusion \\(S_k\\) represents the inferential relationship of \\(S_k\\) being implied by the conjunction of \\(s_i\\) and \\(s_j\\).5 Each process of reflective equilibration takes place on the background of one dialectical structure that stays fixed during the process.\nThe final desideratum demands that a theory does not only perform well in systematizing the commitments \\(\\mathcal{C}\\) but is generally able to systematize sentences in \\(S\\) (independent of whether they belong to the agent’s epistemic state). Systematicity \\(S(\\mathcal{T})\\) measures this general inferential potential by considering the amount of \\(\\mathcal{T}\\)’s implications in relation to the size of the sentence pool \\(\\mathcal{S}\\).\nAll three desiderata can “pull” in different directions. The resolution of such trade-offs is modelled by using a convex combination of the three measures as a one-dimensional combined measure \\(Z\\) for the overall epistemic quality of the agent’s epistemic state:\n\\[\nZ(\\mathcal{C}, \\mathcal{T}\\, \\vert\\, \\mathcal{C}_0) = \\alpha_{A}\\cdot A(\\mathcal{C}, \\mathcal{T}) + \\alpha_{S}\\cdot S(\\mathcal{T}) + \\alpha_{F}\\cdot F(\\mathcal{C}\\,\\vert\\, \\mathcal{C}_0),\n\\]\nThe weights \\(\\alpha_{A}\\), \\(\\alpha_{S}\\) and \\(\\alpha_{F}\\) are real-valued numbers between \\(0\\) and \\(1\\) that sum up to \\(1\\). Different suggestions for balancing the desiderata are represented by choosing different \\(\\alpha\\)-weights in the achievement function \\(Z\\).\nThe achievement function assigns to every epistemic state \\((\\mathcal{C}, T)\\) a value of “overall betterness” relative to what we can call an epistemic situation of an agent, i.e., a dialectical structure \\(\\tau\\), a set of initial commitments \\(\\mathcal{C}_0\\), and a configuration of weights \\((\\alpha_{A}, \\alpha_{S}, \\alpha_{F}\\)). The epistemic situation captures the subject matter of inquiry, its background, and decisions to handle trade-offs between epistemic desiderata."
  },
  {
    "objectID": "intro.html#model-variations",
    "href": "intro.html#model-variations",
    "title": "1  Introduction",
    "section": "1.2 Model variations",
    "text": "1.2 Model variations\nIn this report, we compare the performance of four model variants that result from a combination of two independent alterations of the original model from Beisbart, Betz, and Brun (2021) (see Table 1.1). First, we will vary the general shape of the functions \\(A\\), \\(S\\) and \\(F\\). In the original model, these functions have a quadratic form, which will be contrasted with a linear form. Second, we will compare the semi-global optimization during equilibrations steps, which is used in Beisbart, Betz, and Brun (2021), with a locally optimizing model variant.\n\n\n\nTable 1.1: Model variations\n\n\n\n\n\n\n\n\nQuadratic shape\nLinear shape\n\n\n\n\nGlobal optimization\nQuadraticGlobalRE (in short, QGRE)\nLinearGlobalRE (in short, LGRE)\n\n\nLocal optimization\nQuadraticLocalRE (in short, QLRE)\nLinearLocalRE (in short, LLRE)\n\n\n\n\n\n\n1.2.1 Quadratic and Linear Measures\nIn Beisbart, Betz, and Brun (2021), the functions \\(A\\), \\(F\\) and \\(S\\) have the following shape:\n\\[\nG(x)= 1-x^2\n\\]\nHowever, the quadratic term \\(x^2\\) is not motivated. The linear models LGRE and LLRE will be based on \\(G(x)= 1-x\\) to examine the repercussions of such a variation.\n\n\n1.2.2 Semi-globally and locally optimizing equilibration processes\nThe mutual adjustment of commitments and theories involves two types of revisions. The agent will revise their current commitments \\(\\mathcal{C}_i\\) and their current theory \\(\\mathcal{T}_i\\) in an alternating fashion. More specifically, when adjusting their commitments, the agent will search for new commitments \\(\\mathcal{C}_{i+1}\\) such that the resulting state \\((\\mathcal{C}_{i+1}, \\mathcal{T}_i)\\) performs better w.r.t. \\(Z\\). Similarly, when adjusting their theory, the agent will search for a theory \\(\\mathcal{T}_{i+1}\\) such that \\(Z(\\mathcal{C}_{i}, \\mathcal{T}_{i+1}\\,\\vert\\, \\mathcal{C}_0)&gt; Z(\\mathcal{C}_{i}, \\mathcal{T}_{i}\\,\\vert\\, \\mathcal{C}_0)\\).\nThe equilibration process in Beisbart, Betz, and Brun (2021) is a semi-global optimization in the following way: When searching for new commitments \\(\\mathcal{C}_{i+1}\\) that improve \\(Z\\), the agent can choose any set of commitments. Similarly, when searching for a new theory \\(\\mathcal{T}_{i+1}\\), the agents can choose any theory. This search strategy is computationally costly as the search space grows exponentially with the size of the sentence pool. For the same reason, it is also an unrealistic assumption about real epistemic subjects.\nTo solve this problem and incorporate some form of bounded rationality into the model, we can constrain the search space for adopting new commitments and theories. Instead of considering all commitments and theories, a locally optimizing equilibration process confines the search space to a neighbourhood of the current state.\nThe definition of this neighbourhood is based on an edit distance, which measures the number of changes needed to transform one set of sentences into another. Suppose the sentence pool \\(\\mathcal{S}\\) comprise three sentences and their negations—that is, \\(S=\\{s_1,s_3,s_3,\\neg s_1, \\neg s_2, \\neg s_3\\}\\). Let us now consider two different sets of commitments: \\(\\mathcal{C}_1=\\{s_1, \\neg s_2\\}\\) and \\(\\mathcal{C}_2=\\{s_1,s_2,s_3\\}\\). Suppose further that an agent adopts \\(\\mathcal{C}_1\\) as their commitments. In other words, they accept \\(s_1\\), refuse \\(s_2\\) and are indifferent towards \\(s_3\\). Consequently, a set of commitments can be specified by describing the doxastic attitude (acceptance, refusal and indifference) towards each sentence of half the sentence pool (\\(s_1\\), \\(s_2\\) and \\(s_3\\) in our example). The edit distance we use is defined by asking how many changes of doxastic attitudes are needed to transform one set of commitments into another. Consequently, the edit distance between \\(\\mathcal{C}_1\\) and \\(\\mathcal{C}_2\\) is \\(2\\) since we would have to change the attitude for \\(s_2\\) from refusal to acceptance and for \\(s_3\\) from indifference to acceptance.\nWe can now define the neighbourhood of depth \\(d\\) (in short, the \\(d\\)-neighbourhood) of a set of sentences \\(S_i\\) as the set of all sentence sets that have at most an edit distance of \\(d\\) to \\(S_i\\).6\nThe local model variants QLRE and LLRE restrict the commitments and theory candidates during adjustment steps to a neighbourhood of depth \\(d=1\\).\nTo illustrate the difference between global, semi-global and local optimization, think of epistemic states \\((C, T)\\) as cells on an appropriately sized, possibly non-square, chess board.7 The unbounded, globally optimizing agent can overview the entire board at once (Figure 1.2), while a semi-globally optimizing agent can evaluate only a single row or column per adjustment step (Figure 1.3). Finally, only candidates from a small neighbourhood of the current position are available to the locally optimizing agent only during an adjustment step (Figure Figure 1.4).\n\n\n\nFigure 1.2: Global optimization: All epistemic states are available.\n\n\n\n\n\nFigure 1.3: Semi-global optimization: All sets of commitments and all theories are available in an alternating fashion while the other component is held fixed.\n\n\n\n\n\nFigure 1.4: Local optimization (alternating): Available commitments(row)/theories (column) are restricted to a neighbourhood of the current state in an alternating fashion while the other component is held fixed."
  },
  {
    "objectID": "intro.html#sec-intro-metrics",
    "href": "intro.html#sec-intro-metrics",
    "title": "1  Introduction",
    "section": "1.3 Metrics for model validations",
    "text": "1.3 Metrics for model validations\nAt the outset, a plethora of metrics could be used to examine the performance of the formal model. Let us motivate a small selection of desiderata for model validation, which we will use in the following chapters.\nThe process of reflective equilibration reaches an endpoint, a so-called fixed point, if the agent arrives at an epistemic state that cannot be further improved (in terms of the achievement function) by revising their commitments or their theory, respectively (Beisbart, Betz, and Brun 2021, 450). However, such a fixed point is not necessarily a global optimum. In other words, other epistemic states might perform better w.r.t. \\(Z\\).\nThis possible divergence of fixed points and global optima applies to locally optimizing models (LLRE and QLRE) and the semi-globally optimizing models (LGRE and QGRE). The former can get stuck in local optima since they are confined to a restricted search area for the improvement of epistemic states. However, the semi-globally optimizing models can also get stuck in local optima since they do not adjust their commitments and theories simultaneously but alternately. Consequently, there is a conceptual delineation between the axiology (as defined by the achievement function) as a static aspect of RE and the equilibration process as the dynamical aspect of RE.8\nAccordingly, several questions concerning the relationship between fixed points and global optima are relevant to the performance assessment of the model variants. In Chapter 3, we investigate whether fixed points are global optima and, conversely, whether global optima are reachable by equilibration processes.\nThe reached achievement of fixed points and global optima is not the only evaluative perspective on epistemic states. In other words, there are other aspects of evaluating reflective equilibria besides the desiderata of account, systemticity and faithfulness (Beisbart, Betz, and Brun 2021, 448–49).\nThe most ambitious requirement demands that a theory accounts fully and exclusively for the commitments of an epistemic state. Fixed points that are global optima and that additionally satisfy this criterion are called full RE states. In Chapter 4, we investigate whether and under which circumstances fixed points are full RE states. We will also analyze whether theories of global optima fully and exclusively account for their commitments.\nWeaker requirements demand that fixed points or, at least, fixed point commitments are dialectically consistent—that is, consistent with respect to all inferential relationships encoded in the given dialectical structure \\(\\tau\\). Consistency is commonly seen as a necessary condition of coherence. Achieving consistency is, therefore, of utmost importance for equilibration processes. In Chapter 5, we will assess the consistency conduciveness of the different model variants.\nFinally, we will investigate whether global optima and fixed points yield extreme values in the normalized measures \\(A\\), \\(F\\) and \\(S\\) (Chapter 6). The achievement function \\(Z\\) aggregates these measures by using weights to model trade-offs between the desiderate (e.g., give up on faithfulness to increase account). Investigating under which circumstances extreme values are achieved in \\(A\\), \\(F\\), and \\(S\\) might improve our understanding of the involved trade-offs and of the consequences of choosing specific weights."
  },
  {
    "objectID": "intro.html#ensemble-description",
    "href": "intro.html#ensemble-description",
    "title": "1  Introduction",
    "section": "1.4 Ensemble description",
    "text": "1.4 Ensemble description\nThe results of RE processes and their global optima depend on the following inputs:\n\nthe model variant,\nthe dialectical structure \\(\\tau\\) and the sentence pool,\nthe \\(\\alpha\\)-weights for the achievement function and\nthe set of initial commitments\n\nLet us call a specification of these inputs a simulation setup.\nDue to the exponential growth of candidate commitments and theories, which all have to be considered for global optima and semi-global adjustment steps in RE processes, the ensemble includes only four sentence pools with a small number of sentences (\\(12\\), \\(14\\) , \\(16\\), \\(18\\)). We generated \\(50\\) dialectical structures and \\(20\\) sets of random initial commitments for each sentence pool. We used every resulting configuration of dialectical structures and initial commitments (out of \\(4\\,000=4\\cdot 50 \\cdot 20\\) configurations) to run RE processes for every of the described model variants and for \\(36\\) \\(\\alpha\\)-weight configurations.\nFor each of the resulting \\(576\\,000\\) simulation setups, we calculated global optima. Note also that one simulation setup does not necessarily determine a fixed point uniquely. For every step of adjusting a theory (or a set of commitments), the subsequent theory (or set of commitments) is underdetermined if there is more than one candidate that maximizes the achievement function. In such cases, the model will randomly choose the next theory (or set of commitments) (Beisbart, Betz, and Brun 2021, 466). The Python implementation of the model allows us to track each of the resulting branches, which we did for this report.\nHowever, for some simulation setups (\\(XXX\\)), we do not have simulation results. Due to reasons of computational feasibility, we had to set a cut-off point for the number of branches per simulation setup and the number of adjustment steps.9 We chose to limit the number of branches to \\(400\\) and set the maximum number of adjustment steps to \\(100\\). Given these restrictions, the resulting ensemble comprises \\(XXX\\) branches.\n\n\nTable 1.2: Ensemble properties\n\n\n\n\n\n\n\n\n\n\nSetups\nModel variants\nSentence pool sizes\nDialectical structures\nInitial commitments\n\\(\\alpha\\)-weights (resolution)\n\n\n\n\n\\(576\\,000\\)\n\\(4\\)\n\\(12\\), \\(14\\), \\(16\\), \\(18\\)\n\\(4*50\\)\n\\(4*20\\)\n\\(36\\) (\\(0.1\\))\n\n\n\n\nLet us now describe the simulation setups more thoroughly.\n\n1.4.1 \\(\\alpha\\)-weights\nEach \\(\\alpha\\)-weight was varied between \\(0.1\\) and \\(0.8\\) in steps of \\(0.1\\) (i.e., values from \\(0.1, 0.2, \\dots, 0.8\\)). Since \\(\\alpha\\)-weights have to satisfy \\(\\alpha_{A} + \\alpha_{S} + \\alpha_{F} = 1\\), there are \\(36\\) possible combinations of the described \\(\\alpha\\)-weights.\nWe excluded extreme values such as \\(\\alpha_{F} = 0\\) or \\(\\alpha_{A} = 1\\) since they “break” the model and lead to undesirable behaviour. For example, \\(\\alpha_{F} = 0\\) results in global optima that comprise all and only singleton theories and their closures as commitments.\n\n\n\n1.4.2 Initial commitments\nWe generated a simple random sample of \\(20\\) sets of minimally consistent initial commitments for every sentence pool. While we allow initial commitments to be dialectically inconsistent—that is, inconsistent w.r.t. the inferential relationships codified in \\(\\tau\\)— they must be minimally consistent. In other words, they should not include flat contradictions (e.g., \\(\\{s_1,s_2,\\neg s_1\\}\\)).\n\nLet \\(2^\\mathcal{S}\\) be the set of all sets of minimally consistent sets of sentences from \\(\\mathcal{S}\\).10 If \\(2n\\) is the size of the sentence pool, then there are \\(3^{n}\\) minimally consistent sets of sentences (\\(\\vert 2^\\mathcal{S} \\vert = 3^{n}\\)). For the generation of the used random sample, every set of commitments has the same probability of being drawn. Note that this does not translate into a uniform distribution of the number of initial commitments since the amount of sets with a specific size varies in \\(2^\\mathcal{S}\\). In Figure 1.5, you find the actual distribution of the initial commitments’ sizes for the different sentence pools.\n\n\n\nFigure 1.5: Distribution of initial commitments’ sizes for different n.\n\n\nRoughly, \\(56\\%\\) [XXX] of the random initial commitments are dialectically consistent. This value varies slightly depending on the sentence pool (see Figure 1.6).\n\n\n\nFigure 1.6: Relative share of dialectically consistent initial commitments for different n.\n\n\n\n\n1.4.3 Dialectical structures\n\nWe generated \\(50\\) random dialectical structures for each sentence pool, which codify all inferential relationships on which an RE process is based. A dialectical structure comprises arguments with an internal premise-conclusion structure and dialectical relationships between arguments. Arguments can attack or support each other (see Figure 1.7 for an example).11\n\n\n\nFigure 1.7: Example of a dialectical structure. Attack relations are indicated by waved-shaped arrows, and support relations by straight arrows. Numbers represent sentences from \\(S\\), and the minus sign denotes the negation of a sentence.\n\n\nInferential relationships are represented in a dialectical structure \\(\\tau\\) in the following way: If the sentences \\(P=\\{s_ {i_1},s_{i_2},\\dots, s_{i_m}\\}\\) are premises of an argument in \\(\\tau\\) and \\(s_j\\) is its conclusion, \\(s_j\\) is (known to be) implied by \\(P\\). If an argument \\(A\\) supports another argument \\(B\\), the conclusion of \\(A\\) is (known to be) equivalent to a premise of \\(B\\); if an argument \\(A\\) attacks another argument \\(B\\), the conclusion of \\(A\\) is (known to be) inconsistent with a premise of \\(B\\).\nThe inferential density of a dialectical structure \\(\\tau\\) “can be understood as measure of the inferential constraints encoded in \\(\\tau\\)” (Betz 2013, 44) and is defined as \\[\nD(\\tau) = \\frac{n-lg(\\sigma)}{n}\n\\]\nwhere \\(2n\\) is the size sentence pool and \\(\\sigma\\) the number of complete and consistent positions in \\(\\tau\\)\nThe \\(\\tau\\)-generating algorithm we used receives the following parameters as constraints:\n\nthe size of the sentence pool (\\(n\\in \\{6,7,8,9\\}\\)),\nan interval for the permissible number of arguments (\\(I_{\\vert\\tau\\vert}=[n-2,n+2]\\)),\nthe maximum number of premises per argument (\\(P_{n_{max}}=2\\)),\nprobability weights for the number of premises for arguments (i.e., weights for each \\(1,\\dots, P_{n_{max}}\\)) and\nan interval for the permissible inferential density (\\(I_D=[0.15,0.5]\\))\n\nThe algorithm will generate a dialectical structure by randomly constructing arguments so that the number of arguments and the inferential density fall in the specified intervals \\(I_{\\vert\\tau\\vert}\\) and \\(I_D\\). Both properties correlate inversely: Roughly, the more arguments \\(\\tau\\) has, the higher its inferential density.\nBesides the specified parameters, the algorithm will satisfy the following constraints:\n\nThe dialectical structure is satisfiable (i.e., there is at least one dialectically consistent position on \\(\\tau\\)).\nEvery sentence will be used. In other words, for every sentence \\(s\\in \\mathcal{S}\\), there is an argument in \\(\\tau\\) such that \\(s\\) or \\(\\neg s\\) is either a premise or the conclusion of the argument.\n\nArguments are not question-begging (i.e., an argument’s conclusion is not in its premise set).\nArguments are not attack-reflexive (i.e., the negation of an argument’s conclusion is not in its premise set).\n\n\nFigure 1.8 plots the actual distribution of inferential densities in the generated data set of all \\(200\\) dialectical structures.\n\n\n\nFigure 1.8: Distribution of inferential densities in the used \\(\\tau\\)-data set.\n\n\nAll generated dialectical structures have arguments with \\(1\\)-\\(2\\) premises. For each sentence pool, we used five sets of weights for the number of premises such that there are \\(10\\) dialectical structure with an expected number of premises \\(E(\\vert P_\\tau \\vert)\\) of \\(1\\), \\(10\\) with \\(E(\\vert P_\\tau \\vert)=1.25\\), \\(10\\) with \\(E(\\vert P_\\tau \\vert)=1.5\\), \\(10\\) with \\(E(\\vert P_\\tau \\vert)=1.75\\) and \\(10\\) with \\(E(\\vert P_\\tau \\vert)=2\\). The resulting actual distribution of the mean number of premises per argument can be seen in Figure 1.9. The increased amount of \\(\\tau\\)s with only \\(1\\) and \\(2\\)-premise arguments results from ceiling effects since all arguments have at least one and at most two premises.\n\n\n\nFigure 1.9: Distribution of the mean number of premises in the used \\(\\tau\\)-data set."
  },
  {
    "objectID": "intro.html#summary-of-results",
    "href": "intro.html#summary-of-results",
    "title": "1  Introduction",
    "section": "1.5 Summary of results",
    "text": "1.5 Summary of results\nto come …\n\n\n\n\n\n\n\n\n\nBeisbart, Claus, Gregor Betz, and Georg Brun. 2021. “Making Reflective Equlibrium Precise: A Formal Model.” Ergo 8 (0). https://doi.org/10.3998/ergo.1152.\n\n\nBetz, Gregor. 2010. Theorie dialektischer Strukturen. Frankfurt am Main: Klostermann.\n\n\n———. 2013. Debate Dynamics: How Controversy Improves Our Beliefs. Synthese Library. Dordrecht: Springer Netherlands.\n\n\nFreivogel, Andreas. 2023. “Does Reflective Equilibrium Help Us Converge?” Synthese 202 (6): 1–22. https://doi.org/10.1007/s11229-023-04375-0."
  },
  {
    "objectID": "intro.html#footnotes",
    "href": "intro.html#footnotes",
    "title": "1  Introduction",
    "section": "",
    "text": "The results of Beisbart, Betz, and Brun (2021) are based on a Mathematica implementation of the model (see https://github.com/debatelab/remoma). Here, we rely on a reimplementation in Python (rethon), which can reproduce the results of the original implementation (see this notebook).↩︎\nFor a thorough and complete description of the formal RE model, see Beisbart, Betz, and Brun (2021). The present section is based on condensed material from Freivogel (2023).↩︎\nFor formal details of all measures, see (Beisbart, Betz, and Brun 2021, 464–66).↩︎\nThe used distance is a weighted Hamming distance. For details, see Beisbart, Betz, and Brun (2021), 465.↩︎\nThe arguments of a dialectical structure \\(\\tau\\) need not be formally valid, but can include “arguments that are valid given the relevant background theories” (Beisbart, Betz, and Brun 2021, 460). Additionally, \\(\\tau\\) does not need to codify all inferential relationships between sentences in \\(S\\) and can, in this way, model some form of bounded rationality.↩︎\nFor a sentence pool size of \\(2n\\), the number of positions in the neighbourhood of a position is \\(\\sum_{k=0}^{d} \\binom{n}{k} \\cdot 2^k,\\) where \\(d\\) denotes the neighbourhood depth. For \\(d = 1\\), the number of positions in the neighbourhood grows linearly with the number of sentences. More specifically, for \\(d=1\\), the size of the neighbourhood is \\(2n + 1\\).↩︎\nNote that the two-dimensional representation of the epistemic states in the subsequent figures is purely illustrative. There is no inherent linear order among positions, which can be understood as points in an \\(n\\)-dimensional discrete space.↩︎\nThe fact that the model allows distinguishing static and dynamic aspects makes the model a fruitful foil to discuss the broader epistemological questions surrounding the method of reflective equilibrium (Beisbart, Betz, and Brun 2021, 457–58).↩︎\nThe models will always converge within a finite number of steps into a fixed point (see Beisbart, Betz, and Brun 2021, 467). The same could be shown for the number of branches. However, the number of branches and the length of processes can still be computationally challenging.↩︎\n\\(2^\\mathcal{S}\\) is a subset of the powerset of \\(\\mathcal{S}\\), which is usually denoted by \\(2^\\mathcal{S}\\).↩︎\nThe illustrated dialectical structure is one from the actual data set (with the name tau-alpha-020). Argument maps of all used dialectical structures can be found here XXX.↩︎"
  },
  {
    "objectID": "chapter_general-props.html#process-length-and-step-length",
    "href": "chapter_general-props.html#process-length-and-step-length",
    "title": "2  General ensemble properties",
    "section": "2.1 Process length and step length",
    "text": "2.1 Process length and step length\nIn the following, we understand process length (\\(l_p\\)) as the number of theories and commitment sets in the evolution \\(e\\) of the epistemic state, including the initial and final state.\n\\[\n\\mathcal{C_0} \\rightarrow \\mathcal{T_0} \\rightarrow \\mathcal{C_1} \\rightarrow \\mathcal{T_1} \\rightarrow \\dots \\rightarrow \\mathcal{T_{final}} \\rightarrow \\mathcal{C_{final}}\n\\]\nIn other words, if \\((\\mathcal{T_{0}}, \\mathcal{C_{0}})\\) is the initial state and \\((\\mathcal{T_{m}}, \\mathcal{C_{m}})\\) the fixed-point state, \\(l_p(e)=2(m+1)\\). An equilibration process reaches a fixed point if the newly chosen theory and commitments set are identical to the previous epistemic state—that is, if \\((\\mathcal{T_{i+1}}, \\mathcal{C_{i+1}})=(\\mathcal{T_{i}}, \\mathcal{C_{i}})\\) (Beisbart, Betz, and Brun 2021, 466). Therefore, the minimal length of a process is \\(4\\). In such a case, the achievement of initial commitments and the first chosen theory cannot be further improved. Accordingly, the initial commitments are also the final commitments.\nFigure 2.1 shows the distribution of process lengths, and Figure 2.2 shows the mean process length (and its standard deviation) for the different model variants dependent on the size of the sentence pool (\\(2n\\)) over all branches.\n\n\n\nFigure 2.1: Distribution of process lengths for different models.\n\n\nThe first interesting observation is that the semi-globally optimizing models (QGRE and LGRE) reach their fixed points quickly. Often, they adjust their commitments only once (\\(l_p(e)=6\\)); the linear model variant (LGRE) will sometimes not even adjust the initial commitments of processes (\\(l_p(e)=4\\)). In contrast, the locally optimizing models (QLRE and LLRE) need significantly more adjustment steps. This difference is expected if we assume that local and global optima commitments are not often in the \\(1\\)-neighbourhood of initial commitments (see Figure 2.4 and Figure 2.8). Under this assumption, the locally searching models will need more than one adjustment step to reach a global or local optimum.\n\n\n\nFigure 2.2: Mean process length for different models and sentence pools.\n\n\nAdditionally, the models QLRE and LLRE have a much larger variance in process lengths than the models QGRE and LGRE.\nA third observation concerns the difference in process lengths between semi-globally and locally optimizing models in terms of their dependence on the sentence pool. Figure 2.2 suggests that the process length of locally optimizing models increases with the size of the sentence pool. The semi-globally optimizing models lack such a dependence on the sentence pool size.\nA possible explanation is motivated by analyzing the step length during the adjustment of commitments. Figure 2.3 shows the mean distance between adjacent commitments sets in the evolution of epistemic states over all branches. For simplicity, we measure the distance between two commitment sets by their simple Hamming distance, defined as the number of sentences not shared by both sets. For example, the simple Hamming distance between the commitments sets \\(\\{s_1,s_2\\}\\) and \\(\\{s_2,s_3\\}\\) is \\(2\\) since there are two sentences (\\(s_1\\) and \\(s_3\\)) not shared by both sets.\n\n\n\nFigure 2.3: Mean step length of adjacent commitments for different models and sentence pools.\n\n\nUnsurprisingly, the locally optimizing models have roughly a mean step length of \\(1\\) since they are confined in their choice of new commitments to the \\(1\\)-neighbourhood.1 In contrast, the semi-globally optimizing models take bigger leaps with an increasing sentence pool size. Figure 2.4 shows why: With the increasing size of the sentence pool, the mean distance between initial commitments and fixed-point commitments increases. In other words, RE processes must overcome larger distances to reach their equilibrium state. Semi-globally optimizing models can walk this distance with fewer steps (Figure 2.2) since they can take comparably large steps (Figure 2.3). Locally optimizing models are confined to small steps (Figure 2.3) and, thus, have to take more steps (Figure 2.2).\n\n\n\nFigure 2.4: Mean distance between initial commitments and fixed points."
  },
  {
    "objectID": "chapter_general-props.html#global-optima",
    "href": "chapter_general-props.html#global-optima",
    "title": "2  General ensemble properties",
    "section": "2.2 Global optima",
    "text": "2.2 Global optima\nGlobal optima are fully determined by the achievement function of the RE model. Accordingly, global optima might differ between the linear and quadratic model variants but do not depend on whether the RE process is based on a locally or semi-globally optimization. In the following, we will therefore summarize analysis results with respect to global optima for linear models under the heading LinearRE and for quadratic models under the heading QuadraticRE.2\nThe mean number of global optima does not differ significantly between linear and quadratic models (\\(4\\pm 14\\) vs. \\(4\\pm 9\\) [XXX]) and does not depend on the size of the sentence pool (see Figure 2.5).\n\n\n\nFigure 2.5: Number of global optima for different \\(n\\).\n\n\nHowever, the heatmap in Figure 2.6 shows an interesting dependence on the \\(\\alpha\\)-weights.\nHere and in the following chapters, we will often rely on such heatmaps. Let us therefore provide some clarifications of their interpretation. If we are interested in visualising the dependence on \\(\\alpha\\)-weight configurations (i.e., a specific triples of \\(\\alpha_A\\), \\(\\alpha_F\\) and \\(\\alpha_S\\)), it is sufficient to use two dimensions (\\(\\alpha_A\\) and \\(\\alpha_S\\) in our case) since the three weights \\(\\alpha_A\\), \\(\\alpha_F\\) and \\(\\alpha_S\\) are not independent. The diagonals in these heatmaps from southwest to northeast are isolines for the faithfulness weight (\\(\\alpha_F\\)). In the following, we will refer to specific cells in these heatmaps in the typical \\((x,y)\\) fashion. For instance, we will call the cell with \\(\\alpha_S=0.5\\) and \\(\\alpha_A=0.2\\) the \\((0.5,0.2)\\) cell.\nNow, let’s come back to Figure 2.6. For each simulation setup there is not necessarily one global optimum. Instead, there can be multiple global optima. Each cell in the heatmap provides for a specific \\(\\alpha\\)-configuration setup the mean number of global optima of simulation setups with this \\(\\alpha\\) configuration. For the quadratic models, the number of global optima (and its variance) increases with an increase in \\(\\alpha_S\\). For the linear models, on the other hand, the number of global optima is comparably low (\\(1-3\\)) in all cells with the exception of the three islands \\((0.4,0.3)\\), \\((0.6,0.2)\\) and \\((0.8,0.1)\\). These cells are characterised by \\(\\alpha_F = \\alpha_A\\). For linear models, there are more ties in the achievement function under these conditions, which results in an increase in global optima.\n\n\n\nFigure 2.6: Mean number of global optima for different \\(\\alpha\\)-weight configurations.\n\n\nThe following figures provide an overview of two topological properties of global optima. Figure 2.7 and Figure 2.9 depict the mean distance of global-optimum commitments. We calculated for each configuration setup that has more than one global optimum the mean (simple Hamming) distance between global-optimum commitments and took the average of these means with respect to different ensemble properties. Figure 2.8 and Figure 2.10, one the other hand, depict the mean distance between initial commitments and global-optimum commitments. For that, we calculated for each simulation setup the mean (simle Hamming) distance between initial commitments and all global-optimum commitments of the simulation setup and, again, took the average of these means with respect to different ensemble properties.\nFigure 2.7 and Figure 2.9 are hard to interpret. The mean distance of initial commitments and global-optimum commitments might increase with the size of the sentence pool. However, without an additional consideration of larger sentence pool, we cannot draw this conclusion with certainty (due to the large variance). It is similarly difficult to read of any dependence of the mean distance between global optima on the size of the senctence pool.\n\n\n\nFigure 2.7: Mean distance of global-optima commitments for different \\(n\\).\n\n\n\n\n\nFigure 2.8: Mean distance between initial commitments and optimal commitments for different \\(n\\).\n\n\nFigure 2.8 and Figure 2.10, one the other hand, show that the mean distance of initial commitments and global-optimum commitments as well as the mean distance between global-optimum commitments depend on \\(\\alpha_F\\). The smaller \\(\\alpha_F\\), the larger the distance. This result is not suprising. The weight \\(\\alpha_F\\) determines the extent to what final commitments should resemble initial commitments. You can think of \\(\\alpha_F\\) as the magnitude of an attractive force that pulls the commitments of the epistemic state to the initial commitments. Accordingly, if \\(\\alpha_F\\) gets smaller, global optima and fixed points will be distributed more widerspread in the space of epistemic states.\n\n\n\nFigure 2.9: Mean distance of global-optima commitments for different \\(\\alpha\\).\n\n\n\n\n\nFigure 2.10: Mean distance between initial commitments and optimal commitments for different \\(\\alpha\\)."
  },
  {
    "objectID": "chapter_general-props.html#branching",
    "href": "chapter_general-props.html#branching",
    "title": "2  General ensemble properties",
    "section": "2.3 Branching",
    "text": "2.3 Branching\nThe choice of a new theory (or a new set of commitments respectively) is underdetermined if there are different candidate theories (or commitment sets) that maximize the achievement of the accordingly adjusted epistemic state. In such a case, the model randomly chooses the new epistemic state. The model we use is able to track all these different branches to assess the degree of this type of underdetermination and to determine all possible fixed points for each configuration setup.\n\n\n\nFigure 2.11: Mean number of branches for different models and sentence pools.\n\n\nFigure 2.11 shows the mean number of branches with their dependence on the model and sentence pool. It suggests that branching is more prevalent in locally optimizing models. The large variance can be partly explained by the heat maps in Figure 2.12, which depict mean values (and standard deviations) for different weight combinations.\nFor LinearGlobalRE there are, again, islands with many branches (the cells \\((0.4,0.3)\\), \\((0.6,0.2)\\) and \\((0.8,0.1)\\)) which are characterised by \\(\\alpha_F = \\alpha_A\\). The high number of branches correlates with a high number of fixed points (compare Figure 2.13) and a high number of global optima within these cells (compare Figure 2.6). We might, therefore, hypothesize that the model produces a high number of branches in these cells due to the high number of global optima.3\n\n\n\nFigure 2.12: Mean number of branches for different models and weights.\n\n\nInterestingly, the identified hotspots of branches (and fixed points) for the LinearGlobalRE model are not reproduced by its locally optimizing cousin. This suggests that the LinearLocalRE model will perform worse than the LinearGlobalRE model to reach the increased amount of global optima.4\nThe “\\(\\alpha_F=\\alpha_A\\)”-line is, however, also relevant for the LinearLocalRE model. Above that line, branching is comparably low (roughly \\(1-3\\)) and below that line comparably high (with a high variance). The high number of branches does, however, not correlate with a high number of fixed points (see Figure 2.13). In other words, a lot of these branches end up in the same fixed point. This behaviour is to some extent even observable in the QuadraticLocalRE model.\n\n\n\nFigure 2.13: Mean number of fixed points for different models and weights.\n\n\n\n\n\n\n\n\n\nBeisbart, Claus, Gregor Betz, and Georg Brun. 2021. “Making Reflective Equlibrium Precise: A Formal Model.” Ergo 8 (0). https://doi.org/10.3998/ergo.1152."
  },
  {
    "objectID": "chapter_general-props.html#footnotes",
    "href": "chapter_general-props.html#footnotes",
    "title": "2  General ensemble properties",
    "section": "",
    "text": "The mean distance is, for some cases, slightly greater than \\(1\\), which can be simply explained: The definition of the \\(1\\)-neighbourhood is based on another Hamming distance than the one used here. In particular, there are sentence sets in the \\(1\\)-neighbourhood of a sentence set whose simple Hamming distance is greater than \\(1\\). For instance, the set \\(\\mathcal{C}_1=\\{s_1, \\neg s_2\\}\\) is in the \\(1\\)-neighbourhood of the sentence set \\(\\mathcal{C}_2=\\{s_1,s_2\\}\\) since it only needs an attitude change towards one sentence (i.e., an attitude change towards \\(s_2\\) from rejection to acceptance). However, the simple Hamming distance is \\(2\\) since both \\(s_2\\) and \\(\\neg s_2\\) are not shared by \\(\\mathcal{C}_1\\) and \\(\\mathcal{C}_2\\).↩︎\nIn our data set, the analysis results might differ between semi-globally and locally optimizing models, which is, however, an artifact of the difference in erroneous model runs (i.e., mode runs that could not properly end (see XXX)). For the subsequent analysis of global optima, we rely on the model results of QuadraticGlobalRE and LinearGlobalRE since it had fewer erroneous model runs.↩︎\nIn Chapter 3, we will analyze to what extent the model is able to reach these global optima. The numbers (\\(7/8/8\\) branches and fixed points and \\(11/32/25\\) global optima) suggest that the number of fixed point are nevertheless not enough to reach all these global optima (see, e.g., Figure 3.6 and Figure 3.14 in Chapter 3).↩︎\nA hypothesis we will scrutinize in Chapter 3 (see, e.g., Figure 3.6 and Figure 3.14).↩︎"
  },
  {
    "objectID": "chapter_go-and-fp.html#background",
    "href": "chapter_go-and-fp.html#background",
    "title": "3  Global optima and fixed points",
    "section": "3.1 Background",
    "text": "3.1 Background\nGlobal optima are epistemic states (i.e., commitments-theory pairs) that maximize epistemic desiderata as the achievement function defines it. The models we assess simulate RE processes by mutually adjusting commitments and theories. Since these models proceed in a semi-globally or locally optimizing fashion, fixed points of RE processes are not necessarily global optima (see Section 1.3 for details). It is, therefore, important to assess the performance of the different models with respect to their ability to reach global optima. Two main questions guide the following evaluation:\n\nGO efficiency: Are fixed points global optima? More specifically, what is the share of global optima among fixed points?\nGO reachability: Are global optima reachable by RE processes? More specifically, what is the share of fixed points that are global optima among global optima?\n\nGO efficiency and reachability might not only differ between model variants but might, additionally, depend on the specifics of the simulation setups. In the following, we will confine the consideration to the following dimensions:\n\nHow do GO efficiency and reachability depend on the size of the sentence pool?\nHow do GO efficiency and reachability depend on the arguments’ mean number of premises?\nHow do GO efficiency and reachability depend on \\(\\alpha\\) weights?\n\nWe will answer these questions by calculating different relative shares in the following way.\nLet the ensemble \\(E\\) be the entirety of simulation setups we used to simulate RE processes. Each simulation setup \\(e\\in E\\) corresponds to a set of RE processes that can evolve with this specific setup. Remember that the different steps in the evolution can be underdetermined. In other words, a RE process might branch. We will denote the set of all branches of a specific simulation setup \\(e\\) with \\(B_e\\). Consequently, a specific setup can have more than one fixed point. Similarly, there is not necessarily one global optimum for each simulation setup but possibly many (denoted by \\(GO_e\\)).\nGO efficiency can be calculated in two different ways. First, we can assess the share of global optima among all branches. In other words, we count those branches in \\(B_e\\) that end up in global optima and divide by \\(|B_e|\\). We will refer to this type of GO efficiency as GO efficiency from the process perspective. However, different branches might end up in the same fixed points. Another way of calculating GO efficiency—GO efficiency from the result perspective—avoids a possible “multiple” counting of fixed points by considering the (mathematical) set of fixed points.\nMore formally, let \\(\\{FPGO\\}_e\\) the set of all fixed points of \\(e\\) that are global optima, and let \\([FPGO]_e\\) the fixed points of all branches in \\(e\\) that are global optima. The latter is formally a multiset, which can contain one fixed point multiple times. We can now define different types of GO efficiency—one based on \\(\\{FPGO\\}_e\\) and one on \\([FPGO]_e\\). The corresponding share will be calculated by formulas of the form\n\\[\nGOE^{proc}(E^*):=\\frac{\\sum_{e\\in E^*}\\vert [FPGO]_e\\vert}{\\sum_{e\\in E^*}\\vert B_e\\vert}\n\\]\nand of the form\n\\[\nGOE^{res}(E^*):=\\frac{\\sum_{e\\in E^*}\\vert \\{FPGO\\}_e\\vert}{\\sum_{e\\in E^*}\\vert \\{FP\\}_e\\vert}\n\\]\nwith respect to different subsets \\(E^* \\subset E\\).\nFor instance, let \\(E_{M_1}\\) be the set of all simulation setups belonging to the model \\(M_1\\). We can calculate the overall GO efficiency of \\(M_1\\) from the process perspective by \\(GOE^{proc}(E_{M_1})\\) and from the result perspective by \\(GOE^{res}(E_{M_1})\\).\nHow can we interpret these different types of GO efficiency? One idea is to interpret them probabilistically. According to this suggestion, the ensemble-based model assessment informs us about the probabilities of catching global optima by means of RE processes. On this view, GO efficiency from the process perspective is the probability of a process ending up in a global optimum. On the other hand, GO efficiency, from the result perspective, is the probability of a fixed point being a global optimum. You can think of the difference in terms of when or under which conditions to ask about the probability. In contrast to the latter case, you do not know the fixed point of the process (perhaps the process has not ended yet) in the former case.\nIt does not make much sense to distinguish GO reachability between the process and result perspective. GO reachability asks about the share of global optima that are reachable by RE processes among all global optima. Naturally, the denominator is the (mathematical) set of all global optima in a simulation setup (\\(GO_e\\)), which is a process-independent property of the simulation setup. Since it might happen that \\(\\vert [FPGO]_e \\vert&gt;\\vert GO_e \\vert\\) we should define GO reachability based on \\(\\{FPGO\\}_e\\):\n\\[\nGOR_{E^*}:=\\frac{\\sum_{e\\in E^*}\\vert \\{FPGO\\}_e\\vert}{\\sum_{e\\in E^*}\\vert GO_e\\vert}\n\\]"
  },
  {
    "objectID": "chapter_go-and-fp.html#results",
    "href": "chapter_go-and-fp.html#results",
    "title": "3  Global optima and fixed points",
    "section": "3.2 Results",
    "text": "3.2 Results\n\n3.2.1 Model Overview\nTable 3.1 and Figure 3.1 provide an overview of the different models’ overall GO efficiency and reachability.\n\n\n\n\nTable 3.1: Overall GO efficiency and reachability of the different models\n\n\n\n\n\n\n\n\nModel\nGO efficiency (result perspective)\nGO efficiency (process perspective)\nGO reachability\n\n\n\n\nLinearGlobalRE\n0.73\n0.73\n0.33\n\n\nLinearLocalRE\n0.45\n0.54\n0.14\n\n\nQuadraticGlobalRE\n0.76\n0.75\n0.49\n\n\nQuadraticLocalRE\n0.33\n0.35\n0.27\n\n\n\n\n\n\nThe semi-globally optimizing models perform on all measures better than the locally optimizing models.\nGO efficiency is high for the former (\\(0.73-0.76\\)) and does not (much) differ between the process and result perspectives. GO efficiency varies for locally optimizing models between \\(0.33\\) and \\(0.54\\). We only observe a difference between the process and result perspective for the LinearLocalRE model (\\(0.45\\) vs. \\(0.54\\)). In other words, the extent of branching for the LinearLocalRE model differs between those processes that end up in global optima and those which do not.\nGO reachability is for all models below GO efficiency and varies between low (\\(0.14\\) for LinearLocalRE) and medium (\\(0.49\\) for QuadraticGlobalRE).\nWith respect to the overall GO efficiency and reachability, the QuadraticGlobalRE model performs best since it reaches the highest value in GO reachability and is slightly better than LinearGlobalRE concerning GO efficiency.\nFor the locally optimizing models, the comparison between quadratic and linear shaped \\(G\\) functions is less clear-cut: While LinearLocalRE performs better in GO efficiency than QuadraticLocalRE (\\(0.45/0.54\\) vs. \\(0.33/0.35\\)), it is the other way around concerning GO reachability (\\(0.14\\) vs. \\(0.27\\)).\n\n\n\nFigure 3.1: Overall GO efficiency (result perspective) and reachability of the different models.\n\n\n\n\n3.2.2 GO efficiency\n\n3.2.2.1 Dependence on sentence pool\nFigure 3.2 shows that GO efficiency is more or less stable along different sizes of the sentence pool for semi-globally optimizing models. The locally optimizing models not only perform worse than the semi-globally optimizing, but GO efficiency decreases for them with an increase in the size of the sentence pool.\n\n\n\nFigure 3.2: Dependence of GO efficiency (result perspective) on the size (\\(2n\\)) of the sentence pool.\n\n\nAs we already saw in the model overview, there is no big difference between the result and process perspective except for the LinearLocalRE model, which performs better from the process than from the result perspective (see Figure 3.3).\n\n\n\nFigure 3.3: Dependence of GO efficiency (process perspective) on the size (\\(2n\\)) of the sentence pool.\n\n\n\n\n3.2.2.2 Dependence on mean number of premises\nFigure 3.4 and Figure 3.5 show the dependence of GO efficiency on the mean number of argument’s premises. They might be interpreted as suggesting that the locally optimizing models tend to perform worse with an increasing amount of premises in arguments. At least the difference between semi-globally and locally optimizing models is smaller for lower mean numbers of premises.\nThe zigzag shape of the lines suggests that the actual underlying variance is bigger than the pictured error bars.1 One explanation might be that GO efficiency depends crucially on properties of the dialectical structures other than the mean number of premises. Since there are few dialectical structures for individual data points, their calculation is hardly based on a representative sample. Accordingly, the zigzag might indicate the variation in GO efficiency more accurately. Consequently, the plots must be interpreted with caution.\n\n\n\nFigure 3.4: Dependence of GO efficiency (result perspective) on the mean number of arguments’ premises.\n\n\n\n\n\nFigure 3.5: Dependence of GO efficiency (process perspective) on the mean number of arguments’ premises.\n\n\n\n\n3.2.2.3 Dependence on \\(\\alpha\\)-weights\nIn the preceding sections, we aggregated over the spectrum of different \\(\\alpha\\)-weight configurations. The question is to what extent GO efficiency depends on the chosen \\(\\alpha\\) weights.\nThe heatmaps in Figure 3.6 and Figure 3.7 provide an overview of the \\(\\alpha\\)-weight dependence. In the following, we will refer to specific cells in the typical \\((x,y)\\) fashion. For instance, we will call the cell with \\(\\alpha_S=0.5\\) and \\(\\alpha_A=0.2\\) the \\((0.5,0.2)\\) cell.\nGO efficiency tends to increase with a decrease in \\(\\alpha_A\\) and with an increase in \\(\\alpha_S\\). There are some exceptions to this pattern, especially in linear models. Most notably, there are four “cold” islands in the linear models from both perspectives (compare the \\((0.2,0.4)\\), \\((0.4,0.3)\\), \\((0.6,0.2)\\) and \\((0.8,0.1)\\) cells in Figure 3.6 and Figure 3.7). The comparably dinimished magnitude of GO efficiency can be explained by the comparably high number of global optima in three of theses cells (compare the \\((0.4,0.3)\\), \\((0.6,0.2)\\) and \\((0.8,0.1)\\) cells in Figure 2.6). Surprisingly, the locally and semi-globally optimizing model perform similarly bad, although the semi-globally optimizing model produces much more branches and fixed points in these cells (compare Figure 2.12 and Figure 2.13). [XXX: Re-check with full data set!]\nAdditionally, linear models tend to exhibit more extreme values than quadratic models. In other words, the difference between “hot” and “cold” regions is higher for linear models than for the quadratic counterparts.\n\n\n\nFigure 3.6: Dependence of GO efficiency (result perspective) from \\(\\alpha\\)-weights for the different model variants.\n\n\n\n\n\nFigure 3.7: Dependence of GO efficiency (process perspective) from \\(\\alpha\\)-weights for the different model variants.\n\n\nFigure 3.8 and Figure 3.9 can be used to compare semi-globally with locally optimizing models. It shows for each \\(\\alpha\\) cell the difference in GO efficiency between the semi-globally optimizing model and its locally optimizing variant. As already observed above, the locally optimizing models perform on average worse than the semi-globally optimizing models. The difference in performance is smaller between the linear variants than the quadratic variants. The LinearLocalRE model is for some \\(\\alpha\\)-weight combinations even better than the LinearGlobalRE and for many configurations as good as the latter.\n\n\n\nFigure 3.8: Comparing GO efficiency (result perspective) between semi-globally and locally optimizing models for different \\(\\alpha\\)-weights.\n\n\n\n\n\nFigure 3.9: Comparing GO efficiency (process perspective) between semi-globally and locally optimizing models for different \\(\\alpha\\)-weights.\n\n\nFigure 3.10 and Figure 3.11 show, additionally, the dependence on the mean number of arguments. The mean number of premises varies between \\(1\\) and \\(2\\). We divided this interval into four bins (\\(1-1.25\\), \\(1.25-1.5\\), \\(1.5-1.75\\) and \\(1.75-2\\)) and every heatmap row aggregates over those dialectical structures that have a mean number of premises in the corresponding bin.\nInterestingly, there is a difference between quadratic and linear models. For the linear models, the heatmaps do not change much with an increase in the mean number of premises. However, heatmaps suggest such a dependence for the quadratic models: The higher the mean number of premises, the higher the difference between semi-globally and locally optimizing models.\n\n\n\nFigure 3.10: Comparing GO efficiency (result perspective) between semi-globally and locally optimizing models for different \\(\\alpha\\)-weights and intervals of the mean number of arguments’ premises.\n\n\n\n\n\nFigure 3.11: Comparing GO efficiency (process perspective) between semi-globally and locally optimizing models for different \\(\\alpha\\)-weights and different intervals of the mean number of arguments’ premises.\n\n\n\n\n\n3.2.3 GO reachability\n\n3.2.3.1 Dependence on sentence pool\nFigure 3.12 shows that GO reachability drops quickly for the linear models and slightly for the quadratic ones with increasing size of the sentence pool. For \\(n=9\\), a locally optimizing model (QuadraticLocalRE) even outperforms a semi-globally optimizing model (LinearGlobalRE).\n\n\n\nFigure 3.12: Dependence of GO reachability on the size (\\(2n\\)) of the sentence pool.\n\n\n\n\n3.2.3.2 Dependence on mean number of premises\nAs before, the overall performance in dependence on the mean number of premises is hard to interpret. Figure 3.13 might suggest that the three models LinearGlobalRE, QuadraticLocalRE and LinearLocalRE perform worse with an increase in the mean number of premises. Only QuadraticGlobalRE is able to keep its level of performance.\n\n\n\nFigure 3.13: Dependence of GO reachability on the mean number of arguments’ premises.\n\n\n\n\n3.2.3.3 Dependence on \\(\\alpha\\)-weights\nThe dependence of GO reachability on \\(\\alpha\\) weights is somewhat similar to that of GO efficiency. For the semi-globally optimizing models, GO reachability tends to increase with a decrease in \\(\\alpha_A\\) and an increase in \\(\\alpha_S\\). Again, there are exceptions to this behaviour. Besides the islands of the LinearGlobalRE model, the \\(0.1\\) \\(\\alpha_F\\) isoline has particularly low GO reachability values for the QuadraticGlobalRE model.\nThe linear model variants’ cold islands can, again, be explained by the comparably high number of global optima in three of theses cells (compare the \\((0.4,0.3)\\), \\((0.6,0.2)\\) and \\((0.8,0.1)\\) cells in Figure 2.6).\nThe locally optimizing model variants have a comparably non-regular dependence on \\(\\alpha\\) weights. Additionally, the values do not vary that much between different cells as compared to the globally optimizing models.\n\n\n\nFigure 3.14: Dependence of GO reachability from \\(\\alpha\\)-weights for the different model variants.\n\n\nThe direct comparison between semi-globally and locally optimizing models (Figure 3.15) shows that locally optimizing models are, for some \\(\\alpha\\)-weight combinations, able to outperform the semi-globally optimizing models (cells with negative values).\n\n\n\nFigure 3.15: Comparing GO reachability between semi-globally and locally optimizing models for different \\(\\alpha\\)-weights.\n\n\nBy separating dialectical structures according to their mean number of premises (Figure 3.16) we can see that the advantage of \\(\\alpha\\)-weight combinations for which the semi-globally optimizing models are on average particularly better w.r.t. GO reachability (roughly, the “hot” cells in the \\((0.2-0.7, 0.1-0.2)\\) area in Figure 3.15) is mainly generated by dialectical structures with a higher mean number of premises. In contrast, cells in which locally optimizing models outperform semi-globally optimizing models (roughly, the “cold” cells of the \\(0.1/0.2\\) \\(\\alpha_F\\) isolines in Figure 3.15) are more or less stable for the different bins for the mean number of premises.\n\n\n\nFigure 3.16: Comparing GO reachability between semi-globally and locally optimizing models for different \\(\\alpha\\)-weights and different intervalls of the mean number of arguments’ premises."
  },
  {
    "objectID": "chapter_go-and-fp.html#conclusion",
    "href": "chapter_go-and-fp.html#conclusion",
    "title": "3  Global optima and fixed points",
    "section": "3.3 Conclusion",
    "text": "3.3 Conclusion\nOn average, GO efficiency is high for semi-globally optimizing models and medium-high for locally optimizing models. The fact that GO efficiency drops for semi-globally optimizing models with the size of the sentence pool is, to some extent, worrisome since they are intended to be used in scenarios with larger sentence pools, which are computationally too demanding for semi-globally optimizing models. The question is whether their performance can be improved by increasing their search depth \\(d\\).\nHowever, in specific contexts the modeller will choose a specific set of \\(\\alpha\\) weights. We already saw that the performance of the different model varies significantly between different \\(\\alpha\\)-weight configurations. Consequently, the dependence on the sentence pool should be repeated for those regions of \\(\\alpha\\)-weight configurations that are of interest for the modeller. For instance, if we choose to confine the analysis to \\(\\alpha\\)-weight configurations with \\(\\alpha_a &lt; \\alpha_s\\), GO efficiency of the LinearLocalRE is as high as of the models QuadraticGlobalRE and LinearGlobalRE (see XXX).\nSurprisingly, GO reachability is low to medium for all models. Additionally, all but the QuadraticGlobalRE model perform worse with an increase in the size of the sentence pool. A better understanding of this behaviour requires a more detailed analysis, which should be based on a more extensive set of dialectical structures.\nThe QuadraticGlobalRE model outperforms all other models on average. A direct comparison of the locally optimizing models is complicated since it involves a trade-off: While the LinearLocalRE model reaches a higher GO efficiency than the QuadraticLocalRE model, it is the other way around with respect to GO reachability."
  },
  {
    "objectID": "chapter_go-and-fp.html#footnotes",
    "href": "chapter_go-and-fp.html#footnotes",
    "title": "3  Global optima and fixed points",
    "section": "",
    "text": "The error bars are standard deviations, which are calculated by bootstrapping on the used subset \\(E^*\\) in the calculation of \\(GOE(E^*)\\).↩︎"
  },
  {
    "objectID": "chapter_full-re-states.html#background",
    "href": "chapter_full-re-states.html#background",
    "title": "4  Full RE states",
    "section": "4.1 Background",
    "text": "4.1 Background\nA theory-commitment-pair \\((C, T)\\) is a full RE state iff\n\nit is a global optimum according to the achievement function, and\nthe theory \\(T\\) fully and exlusively accounts for the commitments \\(C\\)."
  },
  {
    "objectID": "chapter_full-re-states.html#method",
    "href": "chapter_full-re-states.html#method",
    "title": "4  Full RE states",
    "section": "4.2 Method",
    "text": "4.2 Method\nFormally and irrespecitve of the model variant, full and exclusive account means \\(C = \\overline{T}\\), or equivalenty, \\(A(C, T) = 1\\). During the generation of an ensemble, we can store for every global optima and for every fixed point resulting from a simulation setup, whether it satisfies conditions i) and ii) of full RE states, i) being trivially satisfied by global optima. For the relative shares we can consider ratio between the number of full RE states among global optima (fixed points) and the total number of global optima (fixed points) per configuration."
  },
  {
    "objectID": "chapter_full-re-states.html#results",
    "href": "chapter_full-re-states.html#results",
    "title": "4  Full RE states",
    "section": "4.3 Results",
    "text": "4.3 Results\n\n4.3.1 Overall\n\n\n\n\nTable 4.1: Relative share of full RE states among global optima\n\n\n\n\n\n\n\n\nModel\nNumber of full RE global optima\nNumber of global optima\nRelative share of full RE global optima\n\n\n\n\nQuadraticGlobalRE\n8167\n64147\n0.127\n\n\nLinearGlobalRE\n19430\n59949\n0.324\n\n\nQuadraticLocalRE\n8167\n64147\n0.127\n\n\nLinearLocalRE\n19430\n59949\n0.324\n\n\n\n\n\n\n\n\n\nFigure 4.1: Relative share of full RE states among global optima grouped by model variant\n\n\nObservations\n\nThe relative share of full RE states among global optima is identical for QuadraticGlobalRE and QuadraticLocalRE, as well as for LinearGlobalRE and LinearLocalRE in Table 4.1 and Figure 4.1. This is to be expected because the local model variants rely on their global counterparts to determine global optima.\nThe relative share of full RE states among global optima is substantially higher for linear model variants.\n\n\n\n\n\nTable 4.2: Relative share of full RE states among fixed points (unique)\n\n\n\n\n\n\n\n\nModel\nNumber of full RE fixed points\nNumber of fixed points\nRelative share of full RE fixed points\n\n\n\n\nQuadraticGlobalRE\n4201\n41726\n0.101\n\n\nLinearGlobalRE\n7418\n32210\n0.23\n\n\nQuadraticLocalRE\n3025\n55659\n0.054\n\n\nLinearLocalRE\n4538\n22977\n0.198\n\n\n\n\n\n\n\n\n\nFigure 4.2: Relative share of full RE states among unique fixed points grouped by model variant\n\n\nObservations\n\nThe relative share of unique full RE fixed points (Figure 4.2) is lower than the corresponding results for global optima (Figure 4.1). This is not surprising as fixed points are reached through semi-globalLY or locally optimizing processes, which cover a restricted search space in contrast to global optimization.\n\n\n\n\n\nTable 4.3: Relative share of full RE states among fixed points (all branches)\n\n\n\n\n\n\n\n\nModel\nNumber of full RE fixed points\nNumber of fixed points\nRelative share of full RE fixed points\n\n\n\n\nQuadraticGlobalRE\n4699\n48105\n0.098\n\n\nLinearGlobalRE\n7418\n32218\n0.23\n\n\nQuadraticLocalRE\n12644\n157034\n0.081\n\n\nLinearLocalRE\n55638\n112049\n0.497\n\n\n\n\n\n\n\n\n\nFigure 4.3: Relative share of full RE states among fixed points from all branches grouped by model variant\n\n\nObservations\n\nThe relative share of full RE fixed points from all branches (Figure 4.3) are similar to the corresponding results for unique fixed points (Figure 4.2), for QuadraticGlobalRE, LinearGlobalRE, an QuadraticLocalRE, excepting LinearLocalRE.\nFor LinearLocalRE, the relative share of full RE fixed points is significantly higher when consdering the fixed points from all branches rather than unique fixed points. This means that a relatively higher share of branches lead to full RE fixed points than to non-full-RE fixed points.\nThe relative share of full RE fixed points for LinearLocalRE (Figure 4.3), even exceeds the relative share of full RE global optima for linear model variants (Figure 4.1).\nThe number of fixed points from all branches (Table 4.3) is only slightly higher than the number of unique fixed points (Table 4.2) for QuadraticGlobalRE and LinearGlobalRE. In contrast, the number of fixed points from all branches is substantially higher than the number of unique fixed points for QuadraticLocalRE, and even more so for LinearLocalRE.\n\n\n\n4.3.2 Heatmaps\n\n\n\nFigure 4.4: Relative share of full RE states among global optima grouped by model variant and configuration of weights.\n\n\nObservations\n\nLinear model variants exhibit a “tipping line” (see XXX). For \\(\\alpha_{A} &gt; \\alpha_{F}\\), the relative share of full RE global optima is 1, i.e., all global optima are full RE states.\nQuadrat model variants have a smooth transition between low and high relative shares, and they have a “hotspot” for very high values of \\(\\alpha_{A}\\). This result is made plausible by the fact that full RE states require a maximal value for the measure of account, i.e, \\(A(\\mathcal{C}, \\mathcal{T}) = 1\\). High values for \\(\\alpha_{A}\\) benefit the fulfillment of this requirement.\n\n\n\n\nFigure 4.5: Relative share of full RE states among unique fixed points grouped by model variant and configuration of weights.\n\n\n\n\n\nFigure 4.6: Relative share of full RE states among fixed points from all branches grouped by model variant and configuration of weights.\n\n\nObservations\n\nLinear model variants do not exhibit the tipping line for fixed points (Figure 4.5 and Figure 4.6)\nLinear model variants have high relative shares for low faihtfulness, moderate account and high (but non-extreme) weights for systematicity.\nThere are only small differences between relative share of full RE states among unique fixed points (Figure 4.5) and fixed points from all branches (Figure 4.6).\nQuadratGlobalRE exhibits its highest relative shares of full RE fixed points for moderately high values for \\(\\alpha_{A}\\) and very low values for \\(\\alpha_{S}\\)."
  },
  {
    "objectID": "chapter_full-re-states.html#conclusion",
    "href": "chapter_full-re-states.html#conclusion",
    "title": "4  Full RE states",
    "section": "4.4 Conclusion",
    "text": "4.4 Conclusion\nOverall, the relative shares of full RE states among global optima and fixed points is not overwhelming, but heatmaps reveal combinations of weights for GlobalQuadraticRE, GlobalLinearRE and LinearLocalRE, where the relative share of full RE states among the outputs is acceptable. For QuadraticLocalRE, this holds at least for global optima. However, this does not constitute strong reasons to reject QuadraticLocalRE. Depending on the particular goals of an inquiry with RE, a low relative share of full RE states can be seen as a strength of a model, as it does not render everything into a full RE state."
  },
  {
    "objectID": "chapter_commitment-consistency.html#background",
    "href": "chapter_commitment-consistency.html#background",
    "title": "5  Consistency",
    "section": "5.1 Background",
    "text": "5.1 Background\nConsistency is commonly seen as a necessary condition of coherence. Achieving consistency in RE is, therefore, of utmost importance. In contrast to the desiderata of faithfulness, systematicity and account (see Section 1.1), the desideratum of consistency is not hard-wired into the model. Although the agent is not allowed to choose commitments with flat contradictions (i.e., commitment sets of the form \\(\\{s_i,\\dots, \\neg s_i\\}\\)), they can choose dialectically inconsistent commitments (i.e., commitments that are inconsistent with respect to the inferential relationships encoded in the dialectical structure \\(\\tau\\)). Or, more formally, a dialectically inconsistent set of commitments may maximize the achievement during the step of adjusting commitments. Accordingly, the process might end at a fixed point with dialectically inconsistent commitments. The question is, therefore, whether the explicitly modelled desiderata and the specification of the process are sufficiently conducive towards dialectical consistency.1\nIn this chapter, we analyze the dialectical consistency of inputs and outputs (fixed points and global optima) of RE simulations, which can be examined from three different perspectives:\n\nthe consistency of output commitments\nthe “consistency case” that arises from combining the consistency status of initial and output commitments\nthe consistency of the union of output commitments and theory\n\nConcerning 2., the juxtaposition of initial and output commitments allows for four cases, which are labelled as follows:\n\n\n\n\n\n\n\n\n\nendpoint commitments consistent\nendpoint commitment inconsistent\n\n\n\n\ninitial commitments consistent\nconsistency preserving (CP)\nconsistency eliminating (CE)\n\n\ninitial commitments inconsistent\ninconistency eliminating (IE)\ninconsistency preserving (IP)\n\n\n\nCP Cases preserve or “transfer” consistency between initial and endpoint commitments. In IE cases, inconsistent initial commitments are revised for consistent endpoint commitments. IP cases fail to eradicate initial inconsistencies, and finally, there may be CE cases if inconsistencies are introduced to initially consistent commitments.\nFrom the viewpoint of model consolidation, the cases are interesting and relevant in various respects. High shares of IE cases would speak in favour of the model’s revisionary power and signify progress towards establishing coherence by RE. Frequent IP cases, in turn, would speak against the model’s revisionary power with respect to inconsistent initial commitments. Moreover, this could fuel the objection that RE (or the present model thereof) is overly conservative, such that “garbage in” (inconsistent initial commitments) leads to “garbage out” (inconsistent fixed point/global optimum commitments). High relative shares of CP cases are a desirable feature. Finally, frequent CE cases would be a truly worrisome result, as they would indicate that the model leads to a worsening in terms of consistency."
  },
  {
    "objectID": "chapter_commitment-consistency.html#results",
    "href": "chapter_commitment-consistency.html#results",
    "title": "5  Consistency",
    "section": "5.2 Results",
    "text": "5.2 Results\n\n5.2.1 Consistent Outputs\n\n5.2.1.1 Overall Results\n\n\n\n\nTable 5.1: Relative share of consistent commitments among global optima\n\n\n\n\n\n\n\n\nModel\nRelative share of global optima with consistent commitments\nNumber of global optima with consistent commitments\nNumber of global optima\n\n\n\n\nQuadraticGlobalRE\n0.741\n529359\n714584\n\n\nLinearGlobalRE\n0.771\n540556\n700830\n\n\nQuadraticLocalRE\n0.741\n525490\n709289\n\n\nLinearLocalRE\n0.769\n554525\n721096\n\n\n\n\n\n\n\n\n\n\nTable 5.2: Relative share of consistent commitments among fixed points (result perspective)\n\n\n\n\n\n\n\n\nModel\nRelative share of fixed points with consistent commitments\nNumber of fixed points with consistent commitments\nNumber of fixed points\n\n\n\n\nQuadraticGlobalRE\n0.728\n333436\n458147\n\n\nLinearGlobalRE\n0.726\n227000\n312783\n\n\nQuadraticLocalRE\n0.688\n404941\n588236\n\n\nLinearLocalRE\n0.82\n187163\n228122\n\n\n\n\n\n\n\n\n\n\nTable 5.3: Relative share of consistent commitments among fixed points (process perspective)\n\n\n\n\n\n\n\n\nModel\nRelative share of fixed points with consistent commitments\nNumber of fixed points with consistent commitments\nNumber of fixed points\n\n\n\n\nQuadraticGlobalRE\n0.708\n374476\n528616\n\n\nLinearGlobalRE\n0.726\n227097\n313002\n\n\nQuadraticLocalRE\n0.735\n1463131\n1991852\n\n\nLinearLocalRE\n0.952\n1240692\n1303077\n\n\n\n\n\n\nObservations: Consistent Outputs\n\nOverall, the relative share of consistent output commitments is high for all model variants and output types, roughly ranging from 0.69 to 0.95 \nThe overall relative share of consistent global optima commitments is slightly boosted for linear model variants compared to their quadratic counterparts in Table 5.1.\nThe relative shares of consistent commitments among fixed points (result perspective: Table 5.2, and process perspective: Table 5.3) is slightly lower than the corresponding results for global optima in Table 5.1 for QuadraticGlobalRE, QuadraticLocalRE, and LinearGlobalRE\nLinearLocalRE exhibits substantially higher relative shares of consistent commitments among fixed points (result and process perspective)\nThe number of fixed points reached through different branches (process perspective) in local model variants is substantially higher than for global model variants (Table 5.3)\n\n\n\n5.2.1.2 Results Grouped by Sentence Pool Size\n\n\n\nFigure 5.1: Relative share of global optima with consistent commitments grouped by model variant and sentence pool size\n\n\n\n\n\nFigure 5.2: Relative share of fixed points (result perspective) with consistent commitments grouped by model variant and sentence pool size\n\n\n\n\n\nFigure 5.3: Relative share of fixed points (process perspective) with consistent commitments grouped by model variant and sentence pool size\n\n\nObservations\n\nThe relative share of global optima with consistent commitments slighty decrease for larger sentence pool sizes (Figure 5.4).\nThe closeness of results of QuadraticGlobalRE and QuadraticLocalRE, as well as LinearGlobalRE and LinearLocalRE in Figure 5.4 is due to the fact, that local variants rely on their global counterparts to determine global optima. Differences arise due to the exclusion of different erroneous runs.\nThe relative share of fixed points with consistent commitments slightly decreases for larger sentence pool sizes (both perspectives in Figure 5.5 and Figure 5.6) for QuadraticGlobalRE, QuadraticLocalRE, and LinearGlobalRE.\nIn contrast, for LinearLocalRE, the relative share of fixed points with consistent commitments remains roughly constant (result perspective in Figure 5.5) or sligtly increases (process perspective in Figure 5.6)\n\n\n\n5.2.1.3 Results Grouped by Configuration of Weights\n\n\n\nFigure 5.4: Relative share of global optima with consistent commitments grouped by model variant and configuration of weights. Note that local variants are omitted due to almost analogous results.\n\n\n\n\n\nFigure 5.5: Relative share of fixed points (result perspective) with consistent commitments grouped by model variant and configuration of weights\n\n\n\n\n\nFigure 5.6: Relative share of fixed points (process perspective) with consistent commitments grouped by model variant and configuration of weights\n\n\nObservations\n\nLinear models exhibit a “tipping line” for the relative share of global optima and fixed points with consistent commitments. For \\(\\alpha_{A} &gt; \\alpha_{F}\\), the relative share is consistently 1.0. See Appendix A for an explanation.\nIn contrast, quadratic models show a gradient of smoother transitions between relative shares, increasing with higher weights for \\(\\alpha_{A}\\), and also to some extent with higher weights for \\(\\alpha_{A}\\).\n\n\n\n\n5.2.2 Consistency Cases\nThe results of this section are based on a more fine-grained distinction of cases that depend on the consistency status of initial and output commitments.\nNote that the relative shares of cases have been calculated for consistent and inconsistent initial commitments separately. For example, the relative share of inconsistency eliminating cases (inconsistent input, consistent output) among global optima has been calculated with respect to all global optima that result from inconsistent inital commitments.\nConsequently, the relative share of inconsistency eleminating and inconsistency preserving cases add up to 1.0, and so do the relative shares of consistency preserving and consistency eliminating cases.\n\n5.2.2.1 Overall Results\n\n\n\n\nTable 5.4: Relative share of consistency cases among global optima\n\n\n\n\n\n\n\n\n\n\n\nModel\nRelative share of consistency eliminating cases\nRelative share of consistency preserving cases\nNumber of global optima from consistent initial commitments\nRelative share of inconsistency preserving cases\nRelative share of inconsistency eliminating cases\nNumber of global optima from inconsistent initial commitments\n\n\n\n\nQuadraticGlobalRE\n0.053\n0.947\n386131\n0.501\n0.499\n328453\n\n\nLinearGlobalRE\n0.024\n0.976\n366296\n0.453\n0.547\n334534\n\n\nQuadraticLocalRE\n0.053\n0.947\n384850\n0.504\n0.496\n324439\n\n\nLinearLocalRE\n0.023\n0.977\n372362\n0.453\n0.547\n348734\n\n\n\n\n\n\n\n\n\nFigure 5.7: Relative share of consistency cases among global optima resulting from consistent initial commitments\n\n\n\n\n\nFigure 5.8: Relative share of consistency cases among global optima resulting from inconsistent initial commitments\n\n\n\n\n\n\nTable 5.5: Relative share of consistency cases among fixed points (result perspective)\n\n\n\n\n\n\n\n\n\n\n\nModel\nRelative share of consistency eliminating cases\nRelative share of consistency preserving cases\nNumber of fixed points from consistent initial commitments\nRelative share of inconsistency preserving cases\nRelative share of inconsistency eliminating cases\nNumber of fixed points from inconsistent initial commitments\n\n\n\n\nQuadraticGlobalRE\n0.041\n0.959\n246823\n0.543\n0.457\n211324\n\n\nLinearGlobalRE\n0.016\n0.984\n168946\n0.577\n0.423\n143837\n\n\nQuadraticLocalRE\n0.045\n0.955\n278450\n0.552\n0.448\n309786\n\n\nLinearLocalRE\n0.014\n0.986\n119476\n0.361\n0.639\n108646\n\n\n\n\n\n\n\n\n\nFigure 5.9: Relative share of consistency cases among fixed points (result perspective) from consistent initial commitments\n\n\n\n\n\nFigure 5.10: Relative share of consistency cases among fixed points (result perspective) from inconsistent initial commitments\n\n\n\n\n\n\nTable 5.6: Relative share of consistency cases among fixed points (process perspective)\n\n\n\n\n\n\n\n\n\n\n\nModel\nRelative share of consistency eliminating cases\nRelative share of consistency preserving cases\nNumber of fixed points from consistent initial commitments\nRelative share of inconsistency preserving cases\nRelative share of inconsistency eliminating cases\nNumber of fixed points from inconsistent initial commitments\n\n\n\n\nQuadraticGlobalRE\n0.043\n0.957\n264780\n0.541\n0.459\n263836\n\n\nLinearGlobalRE\n0.016\n0.984\n169026\n0.578\n0.422\n143976\n\n\nQuadraticLocalRE\n0.057\n0.943\n916286\n0.443\n0.557\n1075566\n\n\nLinearLocalRE\n0.006\n0.994\n615748\n0.085\n0.915\n687329\n\n\n\n\n\n\n\n\n\nFigure 5.11: Relative share of consistency cases among fixed points (process perspective) from consistent initial commitments\n\n\n\n\n\nFigure 5.12: Relative share of consistency cases among fixed points (process perspective) from inconsistent initial commitments\n\n\nObservations: Consistency Cases\n\n\nThe relative share of consistency-preserving cases is high for all model variants and output types (?fig-overall-go-ic-cons-rp,Figure 5.9, and Figure 5.11). Consistency-eliminating cases occur very rarely.\nThe relative share of inconsistency preserving cases slightly exceed the inconsistency eliminating cases for global optima and fixed points of QuadraticGlobalRE, QuadraticLocalRE, as well as LinearGlobalRE (?fig-overall-go-ic-incons-rp,Figure 5.10, and Figure 5.12).\nThe result perspective makes clear that the linear local model variant reaches inconsistent output commitments from both consistent and inconsistent initial commitments (Figure 5.9 and Figure 5.10), but the process perspective reveals that only very few branches result in these inconsistent output commitments (Figure 5.11 and Figure 5.12).\n\n\n\n5.2.2.2 Results Grouped by Sentence Pool Size\nInconsistency Eliminating Cases \n\n\n\nFigure 5.13: Relative share of inconsistency eliminating cases among fixed points (result perspective) grouped by model variant and sentence pool size\n\n\n\n\n\nFigure 5.14: Relative share of inconsistency eliminating cases among fixed points (process perspective) grouped by model variant and sentence pool size\n\n\nConsistency Preserving Cases \n\n\n\nFigure 5.15: Relative share of consistency preserving cases among fixed points (result perspective) grouped by model variant and sentence pool size\n\n\n\n\n\nFigure 5.16: Relative share of consistency preserving cases among fixed points (process perspective) grouped by model variant and sentence pool size\n\n\n\nObservations\n\nLinearLocalRE is the only model that tends to perform better with increasing sentence pool sizes with respect to all output types and conistency cases.\n\n\n\n5.2.2.3 Results Grouped by Configuration of Weights\nDue to the fact, that inconsistency eliminating and inconsistency preserving cases, as well as consistency eliminating and consistency preserving cases are complementary, we confine the presentation of results to two cases.\nInconsistency Eliminating Cases\n\n\n\nFigure 5.17: Relative share of inconsistency eliminating cases among global optima grouped by model variant and configuration of weights.\n\n\n\n\n\nFigure 5.18: Relative share of inconsistency eliminating cases among fixed points (result perspective) grouped by model variant and configuration of weights.\n\n\n\n\n\nFigure 5.19: Relative share of inconsistency eliminating cases among fixed points (process perspective) grouped by model variant and configuration of weights.\n\n\nObservations: Inconsistency eliminating cases (IE)\n\nLinear models exhibit a “tipping line” for IE cases among both global optima and fixed points. There are no IE cases where \\(\\alpha_{A} &lt; \\alpha_{F}\\), i.e. initial inconsistencies are never removed. In turn, the relative share of IE cases for \\(\\alpha_{A} &gt; \\alpha_{F}\\) is 1.0, i.e. initial inconsistencies are always removed. See Appendix A for an explanation.\nThe case with non extreme values in linear models occur where \\(\\alpha_{A} = \\alpha_{F}\\).\nIn contrast, quadratic models have smooth transitions. High weights for account and systematicity, resulting in low weights for faithfulness, benefit the relative share of IE cases among global optima and fixed points.  \nThe relative shares of IE cases among fixed points (process perspective) in local model variants (Figure 5.19) are slightly boosted in comparison to the consideration of unique fixed points (result perspectve) (Figure 5.18).\n\nConsistency Preserving Case (CP)\n\n\n\nFigure 5.20: Relative share of consistency preserving cases among global optima grouped by model variant and configuration of weights.\n\n\n\n\n\nFigure 5.21: Relative share of consistency preserving cases among fixed points (result perspective) grouped by model variant and configuration of weights.\n\n\n\n\n\nFigure 5.22: Relative share of consistency preserving cases among fixed points (process perspective) grouped by model variant and configuration of weights.\n\n\nObservations: Consistency Preserving Cases (CP)\n\nOverall, CP cases occur very frequently for all model variants and output types. In turn, the relative shares of CE cases (\\(1.0-CP\\)) are very low.\nLinear models exhibit a “tipping line” for CP cases among both global optima and fixed points. For \\(\\alpha_{A} &gt; \\alpha_{F}\\), consitency is always preserved. In turn, CE cases occur only for \\(\\alpha_{A} \\leq \\alpha_{F}\\).\nThe influence of weight configurations is moderately at best.\n\n\n\n\n\n5.2.3 Consistent Unions\nIn this section, we will analyze the dialectical consistency of whole epistemic states—that is, the union of an epistemic state’s commitments and theory. Since we already analyzed the consistency of fixed point commitments and global optima commitments in isolation, we will count only those inconsistencies that arise by combining commitments and theories. In other words, we will not consider inconsistencies that result from inconsistencies in the commitments.\n\n5.2.3.1 Overall Results\n\n\n\n\nTable 5.7: Relative share of global optima with a consistent union of commitments and theory\n\n\n\n\n\n\n\n\nModel\nRelative share of global optima with a consistent union\nNumber of global optima with a consistent union\nNumber of global optima with consistent commitments\n\n\n\n\nQuadraticGlobalRE\n0.931\n492856\n529359\n\n\nLinearGlobalRE\n0.966\n522055\n540556\n\n\nQuadraticLocalRE\n0.932\n489618\n525490\n\n\nLinearLocalRE\n0.966\n535532\n554525\n\n\n\n\n\n\n\n\n\n\nTable 5.8: Relative share of fixed points (result perspective) with a consistent union of commitments and theory\n\n\n\n\n\n\n\n\nModel\nRelative share of fixed points with a consistent union\nNumber of fixed points with a consistent union\nNumber of fixed points with consistent commitments\n\n\n\n\nQuadraticGlobalRE\n0.915\n305081\n333436\n\n\nLinearGlobalRE\n0.96\n218022\n227000\n\n\nQuadraticLocalRE\n0.893\n361422\n404941\n\n\nLinearLocalRE\n0.973\n182164\n187163\n\n\n\n\n\n\n\n\n\n\nTable 5.9: Relative share of fixed points (process perspective) with a consistent union of commitments and theory\n\n\n\n\n\n\n\n\nModel\nRelative share of fixed points with a consistent union\nNumber of fixed points with a consistent union\nNumber of fixed points with consistent commitments\n\n\n\n\nQuadraticGlobalRE\n0.908\n340059\n374476\n\n\nLinearGlobalRE\n0.96\n218065\n227097\n\n\nQuadraticLocalRE\n0.911\n1333612\n1463131\n\n\nLinearLocalRE\n0.994\n1233142\n1240692\n\n\n\n\n\n\nObservations\n\nThe relative shares of consistent unions of commitments and theory among outputs with consistent commitments is very high for all model variants and output types.\n\n\n\n5.2.3.2 Results Grouped by Sentence Pool Size\n\n\n\nFigure 5.23: Relative share of global optima with a consistent union of commitments and theory grouped by model variant and sentence pool size\n\n\n\n\n\nFigure 5.24: Relative share of fixed points (result perspective) with a consistent union of commitments and theory grouped by model variant and sentence pool size\n\n\n\n\n\nFigure 5.25: Relative share of fixed points (process perspective) with a consistent union of commitments and theory grouped by model variant and sentence pool size\n\n\n\n\n5.2.3.3 Results Grouped by Configuration of Weights\n\n\n\nFigure 5.26: Relative share of global optima with a consistent union of commitments and theory grouped by model variant and configuration of weights\n\n\n\n\n\nFigure 5.27: Relative share of fixed points (result perspective) with a consistent union of commitments and theory grouped by model variant and configuration of weights\n\n\n\n\n\nFigure 5.28: Relative share of fixed points (process perspective) with a consistent union of commitments and theory grouped by model variant and configuration of weights"
  },
  {
    "objectID": "chapter_commitment-consistency.html#conclusion",
    "href": "chapter_commitment-consistency.html#conclusion",
    "title": "5  Consistency",
    "section": "5.3 Conclusion",
    "text": "5.3 Conclusion\n\nOverall, the present ensemble study concerning the three perspectives on the consistency of outputs of RE simulations provides positive results with respect to model variation. The overall relative shares of consistent outputs, inconsistency-eliminating and consistency-preserving cases, as well as consistent unions are satisfactorily high for all model variants.\nAccording to analysing the results further with respect to the sentence pool size, LinearLocalRE seems to have the edge over the other model variants in view of increasing sentence pool sizes. Nonetheless, the severely restricted sample that forms the basis of this report would make an extrapolation to even larger sentence pool sizes a highly speculative matter. Further research in this direction is required.\nIn the more fine-grained analysis according to weigh configurations, we can observe regions of weight configurations that yield desirable behaviour. Moreover, these regions are robust across model variants. This provides at least some motivation to prefer some configurations over others. In particular, it is beneficial to consistency considerations if \\(\\alpha_{A} &gt; \\alpha_{F}\\).\nThere is a notable difference between quadratic and linear model variants (smooth transitions vs. tipping line), but on its own, this does not serve as a criterion to prefer some model variants over others. See the Appendix A for a presentation of analytical results that explain why linear model variants exhibit tipping lines."
  },
  {
    "objectID": "chapter_commitment-consistency.html#footnotes",
    "href": "chapter_commitment-consistency.html#footnotes",
    "title": "5  Consistency",
    "section": "",
    "text": "The main driving force for dialectical consistency is the desideratum of account. Since the choice of new theories is confined to dialectically consistent theories, account will favour commitments that are dialectically consistent.↩︎"
  },
  {
    "objectID": "chapter_extreme-values.html#background",
    "href": "chapter_extreme-values.html#background",
    "title": "6  Extreme values for account and faitfulness",
    "section": "6.1 Background",
    "text": "6.1 Background\nIn this study we examine extreme values from measures of account and faithfulness.\n\\(A(C, T) = 1\\) means that the theory \\(T\\) fully and exclusively accounts for the commitments \\(C\\). Full and exclusive account is a condition for full RE states. Conversely, \\(A(C, T) = 0\\) holds if a theory completely fails to account for commitments, that is, for every sentence in the commitments the closure theory does not contain this sentence, or contradicts the theory.\nThe measure of systematicity for a theory \\(\\mathcal{T}\\) is defined as follows: \\[\nS(\\mathcal{T}) = G(\\frac{\\vert \\mathcal{T}\\vert -1}{\\vert\\overline{\\mathcal{T}}\\vert })\n\\]\nHence, \\(S(\\mathcal{T}) = 1\\) if and only if \\(\\vert \\mathcal{T}\\vert = 1\\), i.e., \\(\\mathcal{T}\\) is a singleton theory, e.g., \\(\\mathcal{T} =\\lbrace s\\rbrace\\). Note that it does not matter whether \\(G\\) is linear or quadratic. Furthermore, \\(S(T) = 0\\) if and only if \\(\\mathcal{T}=\\emptyset\\) by definition.\n\\(F(C \\vert C_{0}) = 1\\) holds if and only if the initial committments \\(C_{0}\\) are a subset of the commitments \\(C\\) (expansions of the initial commitments are not penalized). \\(F(C \\vert C_{0})\\) attains the minimal value of 0, if every sentence of the initial commitments \\(C_{0}\\) is missing in or contradicted by the commitments \\(C\\)."
  },
  {
    "objectID": "chapter_extreme-values.html#method",
    "href": "chapter_extreme-values.html#method",
    "title": "6  Extreme values for account and faitfulness",
    "section": "6.2 Method",
    "text": "6.2 Method\nDuring the ensemble generation, the values for account and faithfulness are calculated for every global optimum and fixed point for each simulation setup. The systematicity value has not been stored, but extreme values can (easily) be determined by considering the length (i.e. the number of sentences) in the theory.\n\nNote that the information about the values of measures has not been stored in the dataframe for fixed points from all branches. Thus, the current study only includes results concerning unique fixed points reached from each simulation setup"
  },
  {
    "objectID": "chapter_extreme-values.html#results",
    "href": "chapter_extreme-values.html#results",
    "title": "6  Extreme values for account and faitfulness",
    "section": "6.3 Results",
    "text": "6.3 Results\n\n6.3.1 Overall\n\n6.3.1.1 Minimal values\nThere is no simulation setup which resulted in a global optimum or a fixed point that has a minimal value for the measure of account, systematicity or faithfulness. Consequently, we can exclude the consideration of minimal values from the subsequent analysis.\nThis is a desirable result, as minimal values for would constitute quite strange behaviour of the model variants. For example, \\(F(C\\,\\vert\\,C_{0}) = 0\\) would mean that an agent completely departed from their initial commitments \\(C_{0}\\), which could amount to the undesirable outcome of changing the subject matter.\n\n\n6.3.1.2 Maximal values\n\n\n\n\nTable 6.1: Absolute and relative numbers of global optima maximizing various desiderata measures.\n\n\n\n\n\n\n\n\n\n\n\n\nModel\nNumber of global optima with maximal account\nNumber of global optima with maximal systematicity\nNumber of global optima with maximal faithfulness\nNumber of global optima\nRelative share of global optima with maximal account\nRelative share of global optima with maximal systematicity\nRelative share of global optima with maximal faithfulness\n\n\n\n\nQuadraticGlobalRE\n8167\n44715\n8185\n64141\n0.127\n0.697\n0.128\n\n\nLinearGlobalRE\n19430\n51305\n20301\n59949\n0.324\n0.856\n0.339\n\n\nQuadraticLocalRE\n8150\n44685\n8183\n64071\n0.127\n0.697\n0.128\n\n\nLinearLocalRE\n19163\n51077\n20297\n59682\n0.321\n0.856\n0.34\n\n\n\n\n\n\n\n\n\nFigure 6.1: Relative shares of global optima maximizing the desiderata measures for account, systematicity and faithfulness\n\n\n\n\n\n\nTable 6.2: Absolute and relative numbers of fixed points maximizing various desiderata measures.\n\n\n\n\n\n\n\n\n\n\n\n\nModel\nNumber of fixed points with maximal account\nNumber of fixed points with maximal systematicity\nNumber of fixed points with maximal faithfulness\nNumber of fixed points\nRelative share of fixed points with maximal account\nRelative share of fixed points with maximal systematicity\nRelative share of fixed points with maximal faithfulness\n\n\n\n\nQuadraticGlobalRE\n7482\n22990\n9244\n41725\n0.179\n0.551\n0.222\n\n\nLinearGlobalRE\n11893\n23756\n17311\n32210\n0.369\n0.738\n0.537\n\n\nQuadraticLocalRE\n8353\n25863\n5100\n55643\n0.15\n0.465\n0.092\n\n\nLinearLocalRE\n14661\n11364\n8136\n22907\n0.64\n0.496\n0.355\n\n\n\n\n\n\n\n\n\nFigure 6.2: Relative shares of unique fixed points maximizing the desiderata measures for account, systematicity and faithfulness\n\n\n\n\n\n6.3.2 Grouped by configuration of weights\n\n6.3.2.1 Account\n\n\n\nFigure 6.3: Relative share of global optima maximizing the measure for account grouped by model variant and configuration of weights.\n\n\n\n\n\nFigure 6.4: Relative share of fixed points maximizing the measure for account grouped by model variant and configuration of weights.\n\n\n\n\n6.3.2.2 Systematicity\n\n\n\nFigure 6.5: Relative share of global optima maximizing the measure for systematicity grouped by model variant and configuration of weights.\n\n\n\n\n\nFigure 6.6: Relative share of fixed points maximizing the measure for systematicity grouped by model variant and configuration of weights.\n\n\n\n\n6.3.2.3 Faithfulness\n\n\n\nFigure 6.7: Relative share of global optima maximizing the measure for faithfulness grouped by model variant and configuration of weights.\n\n\n\n\n\nFigure 6.8: Relative share of fixed points maximizing the measure for faithfulness grouped by model variant and configuration of weights."
  },
  {
    "objectID": "chapter_extreme-values.html#conclusion-old",
    "href": "chapter_extreme-values.html#conclusion-old",
    "title": "6  Extreme values for account and faitfulness",
    "section": "6.4 Conclusion (old)",
    "text": "6.4 Conclusion (old)\nThe frequency of minimal values for account and faithfulness in LDS is extremely small and they occur in one ensemble (07), only. Moreover, the extreme values for faithfulness and systematicity correlate with extreme, coresponding weights (\\(\\alpha_{F} \\leq 0.2\\) and \\(\\alpha_{A}\\leq 0.2\\)).\nThe occurence of singleton theories in global optima and fixed points might make for an intersting exploration in the future."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Beisbart, Claus, Gregor Betz, and Georg Brun. 2021. “Making\nReflective Equlibrium Precise: A Formal\nModel.” Ergo 8 (0). https://doi.org/10.3998/ergo.1152.\n\n\nBetz, Gregor. 2010. Theorie dialektischer\nStrukturen. Frankfurt am Main:\nKlostermann.\n\n\n———. 2013. Debate Dynamics: How Controversy\nImproves Our Beliefs. Synthese Library.\nDordrecht: Springer Netherlands.\n\n\nFreivogel, Andreas. 2023. “Does Reflective Equilibrium Help Us\nConverge?” Synthese 202 (6): 1–22. https://doi.org/10.1007/s11229-023-04375-0."
  },
  {
    "objectID": "appendix_tipping_line.html#proposition-1",
    "href": "appendix_tipping_line.html#proposition-1",
    "title": "Appendix A — The Tipping Line of Linear Model Variants",
    "section": "A.1 Proposition 1",
    "text": "A.1 Proposition 1\nAssume that a dialectical structure \\(\\tau\\) and some initial commitments \\(C_{0}\\) are given. Moreover, assume \\(\\alpha_{A} &gt; \\alpha_{F}\\) for a configuration of weights \\((\\alpha_{A}, \\alpha_{S}, \\alpha_{F})\\) in a linear model variant. Then all global optima (relative to \\(C_{0}\\)) according to the achievement function specified by the configuration of weights are full RE states.\nCorollaries The linear model variants exhibit the following behaviour for \\(\\alpha_{A} &gt; \\alpha_{F}\\):\n\nFor global optima, there no inconsistency preserving cases\nConsistency eliminating cases do not occur for global optima\n\nIn contrast, for \\(\\alpha_{A} &lt; \\alpha_{F}\\) the following holds for the linear model variants:\n\nInconsistency eliminating cases do not occur for global optima\n\nOpen questions\n\nInterestingly, the corollaries almost always hold for fixed points as well. However, an analoguous proposition for fixed points would require a different proof.\n\nProof sketch for Proposition 1\nIntuitively, \\(\\alpha_{A} &gt; \\alpha_{F}\\) means that account trumps faithfulness, which allows to select commitments ignoring faithfulness so that they are fully and exclusively accounted for by a theory.\nAssume that an epistemic state \\((C, T)\\) is a global optimum according to the achievement function \\(Z\\) given some initial commitments \\(C_{0}\\) and a configuration of weights \\((\\alpha_{A}, \\alpha_{S}, \\alpha_{F})\\) such that \\(\\alpha_{A} &gt; \\alpha_{F}\\). We need to show that \\((C, T)\\) is a full RE state, i.e., that \\(T\\) fully and exclusively accounts for \\(C\\) (FEA), or equivalently, \\(A(C, T) = 1\\).\nFor a proof by contradiction, assume that \\[A(C, T)=G(\\frac{D_{0,\\,0.3,\\,1,\\,1}(C,\\overline{T})}{n}) &lt; 1,\\] which holds only if \\(D_{0,\\,0.3,\\,1,\\,1}(C,\\overline{T}) &gt; 0\\). In other words, there is at least one sentence \\(s\\) (negated or unnegated) for which there is a positive contribution to the Hamming distance. In particular, we have the following cases:\n\n\\(\\overline{T}\\) extends \\(C\\) with respect to \\(s\\): \\(+0.3\\)\n\\(\\overline{T}\\) contracts \\(C\\) with respect to \\(s\\): \\(+1\\)\n\\(\\overline{T}\\) and \\(C\\) contradict each other with respect to \\(s\\): \\(+1\\)\n\nEach case of changing \\(C\\) with respect to \\(s\\), yielding new commitments \\(C'\\), impacts the contributions to the Hamming distances for account and faithfulness. Note that systematicity is not affected by changing the commitments.\nThe complete linearity of the achievement function allows to distribute (“push in”) the weights \\(\\alpha_{A}\\) and \\(\\alpha_{F}\\) over the individual contributions of the hamming distances.\nSince the achievement function is optimised for minimal contributions and \\(\\alpha_{A} &gt; \\alpha_{F}\\), it is always more attractive to change the commitments to increase account rather than faithfully respecting the initial commitments. This argument can be repeated for every sentence for which \\(C\\) and \\(\\overline{T}\\) differ.\nIn summary, if \\((C, T)\\) is a global optimum but \\(A(C, T) &lt; 1\\), then there is a position \\((C', T)\\) such that \\(A(C, T) &lt; A(C', T)\\) contradicting \\((C, T)\\) being a global optimum. Consequently, we must have \\(A(C, T) = 1\\), i.e., \\(T\\) accounts fully and exclusively for \\(S\\) (FEA). This shows that \\((C, T)\\) is a full RE state.\nRemark\nNote that this argument does not work for quadratic model variants, and in particular, the default model of Beisbart, Betz, and Brun (2021). Remember that the Hamming distance \\(D\\) is a summation of penalties. Consequently, squaring the hamming distance yields a polynomial expression where every contributing penalty “interferes” by multiplication with the others. This blocks the above strategy of comparing the contributions and distributing the weights \\(\\alpha_{A}\\) or \\(\\alpha_{S}\\) over these expressions. In the quadratic models of the above ensemble study, we can observe a gradual transition between configurations that yield global optima, which exhibit a specific behaviour, to configurations that almost certainly fail in this respect."
  },
  {
    "objectID": "appendix_tipping_line.html#proposition-2",
    "href": "appendix_tipping_line.html#proposition-2",
    "title": "Appendix A — The Tipping Line of Linear Model Variants",
    "section": "A.2 Proposition 2",
    "text": "A.2 Proposition 2\nAssume that a dialectical structure \\(\\tau\\) and some initial commitments \\(C_{0}\\) are given. Moreover, assume \\(\\alpha_{A} &lt; \\alpha_{F}\\) for a configuration of weights \\((\\alpha_{A}, \\alpha_{S}, \\alpha_{F})\\) in a linear model variant. Then, for all global optima (relative to \\(C_{0}\\)) according to the achievement function specified by the configuration of weights the following holds:\n\\[\nF(C\\,\\vert\\,C_{0}) = 1.\n\\]\nProof sketch of Proposition 2\nFor a proof by contradiction, assume that \\((C, T)\\) is a global optimum according to \\(Z\\), but \\(F(C\\,\\vert\\,C_{0}) &lt; 1\\).\nThis holds only if \\(G(\\frac{D_{0,0,1,1}(C_{0}, C)}{n}) &lt; 1\\), i.e. only if \\(D_{0,0,1,1}(C_{0}, C) &gt; 0\\). In other words, there is at least one sentence, for which there is a positive contribution to the Hamming distance. In particular, there are two cases\n\n\\(C\\) contracts \\(C_{0}\\) with respect to \\(s\\): +1 (there is \\(s \\in C_{0}\\), but \\(s\\) and \\(\\neg s\\) are not in \\(C\\))\n\\(C\\) and \\(C_{0}\\) contradict each other with respect to \\(s\\): +1\n\nConsider the impacts on individual contributions to the Hamming distances for account and faithfulness of changing \\(C\\) with respect to \\(s\\), yielding new commitments \\(C′\\), in particular the difference \\(d(C_{0}, C', \\lbrace s, \\neg s\\rbrace) - d(C_{0}, C, \\lbrace s, \\neg s\\rbrace)\\). * denotes the worst cases.\n\n\nSituation i)\nThere is an \\(s\\) in \\(C_{0}\\), but \\(s\\) and \\(\\neg s\\) are not in \\(C\\).\nAdd \\(s\\) to \\(C\\). Result: \\(s\\) in \\(C'\\)\nFaithfulness\nagreement (new) - contraction (old): faithfulness: -1\nAccount\n\nCase: \\(s \\in \\overline{T}\\): agreement (new) - expansion (old): -0.7\nCase \\(\\neg\\in \\overline{T}\\): contradiction (new) - expansion (old): + 0.7\n*Case \\(s\\) and \\(\\neg s\\) not in \\(\\overline{T}\\): contraction (new)- agreement (old): +1\n\nSituation ii)\nw.l.o.g. asssume \\(s\\) in \\(C_{0}\\), \\(\\neg s\\) in \\(C\\).\na: Remove the contradicting element from \\(C\\). Result: \\(\\neg s\\) (and \\(s\\)) are not in \\(C'\\).\nFaithfulness\ncontraction (new) - contradiction (old) : +0\nAccount - \\(s in \\overline{T}\\): expansion (new) - contradiction (old): -0.7 - *Case \\(\\neg s \\in \\overline{T}\\): expansion (new) - agreement (old): +0.3 - Case \\(s\\) and \\(\\neg s\\) not in \\(\\overline{T}\\): agreement(new) - contraction(old): -1\nb:Revise the contradicting element in \\(C\\): Result: \\(s\\) in \\(C'\\)\nFaithfulness agreement (new) - contradiction (old): -1\nAccount - Case: \\(s in \\overline{T}\\): agreement (new) - contradiction (old): -1 - *Case \\(\\neg\\in \\overline{T}\\): contradiction (new) - agreement (old): +1 - Case \\(s\\) and \\(\\neg\\) not in \\(\\overline{T}\\): contraction (new) - contraction (old): +0\nThe complete linearity of the achievement function allows to distribute (push in) the weights \\(\\alpha_{A}\\) and \\(\\alpha{F}\\) over the individual contributions of the hamming distances in \\(Z\\). Hence, the weights also apply to the individual contributions considered above. Moreover, changing the commitments does not affect the systematicity of the theory, i.e. \\(S(T)\\) is identical for \\((C, T)\\) and \\((C', T)\\). Hence, the achievement function is optimised for minimal contributions in the measures for account and faithfulness and \\(\\alpha_{F} &gt; \\alpha{A}\\).\nConsequently, it is always (cases i.a, ii.a and ii.b) more attractive to stay faithful to the initial commitments rather than to change the commitments in order to increase account.\nThis argument can be repeated for every sentence, for which \\(C_{0}\\) and \\(C\\) differ.\nIn summary, if \\((C, T)\\) is a global optimum but it is assumed that \\(F(C\\,\\vert\\, C_{0}) &lt; 1\\), then there is a position \\((C′, T)\\) such that \\(Z(C, T \\,\\vert\\, C_{0}) &lt; Z(C', T \\,\\vert\\, C_{0})\\), contradicting \\((C, T)\\) being a global optimum. Consequently, we must have \\(F(C \\,\\vert\\, C_{0}) = 1\\).\nCorollaries The linear model variants exhibit the following behaviour for \\(\\alpha_{A} &gt; \\alpha_{F}\\):\n\nThe relative share of inconsistency eliminating cases among global optima is 0.0. Explanation: Removing or revising an initial inconsistency requires to deviate from the initial commitments, which is incompatible with maximal faithfulness.\nSimilarly, the relative share of inconsistency preserving cases in this region of weight configurations correspond to the relative share of inconsistent initial commitments.\nIn turn, the relative share of global optima with maximal value for faithfulness is 1.0.\n\nOpen questions\n\nFixed points (unique) also exhibit this behaviour, respectively. Can the proof be adapted to cover such cases as well?\n\n\n\n\n\nBeisbart, Claus, Gregor Betz, and Georg Brun. 2021. “Making Reflective Equlibrium Precise: A Formal Model.” Ergo 8 (0). https://doi.org/10.3998/ergo.1152."
  }
]